{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13264453,
          "sourceType": "datasetVersion",
          "datasetId": 8405666
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib matplotlib numpy optuna pandas lightgbm xgboost catboost pytorch-tabnet scikit-learn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:24:31.088762Z",
          "iopub.execute_input": "2025-10-05T11:24:31.089029Z",
          "iopub.status.idle": "2025-10-05T11:25:44.42699Z",
          "shell.execute_reply.started": "2025-10-05T11:24:31.089003Z",
          "shell.execute_reply": "2025-10-05T11:25:44.426247Z"
        },
        "id": "yo3oTZEUboeA",
        "outputId": "b8cc9a12-87ed-4f65-d44a-89006e44caa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nCollecting pytorch-tabnet\n  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\nDownloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "\n",
        "RANDOM_SEED = 67\n"
      ],
      "metadata": {
        "id": "UqSsZp_xdjDY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:44.428854Z",
          "iopub.execute_input": "2025-10-05T11:25:44.429106Z",
          "iopub.status.idle": "2025-10-05T11:25:52.275429Z",
          "shell.execute_reply.started": "2025-10-05T11:25:44.429083Z",
          "shell.execute_reply": "2025-10-05T11:25:52.274841Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ARTIFACT_ROOT = Path(\"artifacts\")\n",
        "\n",
        "\n",
        "def ensure_dir(path: Path) -> Path:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def reshape_for_meta(array: np.ndarray) -> np.ndarray:\n",
        "    array = np.asarray(array)\n",
        "    if array.ndim == 1:\n",
        "        return array.reshape(-1, 1)\n",
        "    return array\n",
        "\n",
        "\n",
        "def persist_model(model, destination: Path, model_name: str) -> Path:\n",
        "    ensure_dir(destination.parent)\n",
        "    if model_name == \"lightgbm\":\n",
        "        file_path = destination.with_suffix(\".txt\")\n",
        "        model.save_model(str(file_path))\n",
        "    elif model_name == \"catboost\":\n",
        "        file_path = destination.with_suffix(\".cbm\")\n",
        "        model.save_model(str(file_path))\n",
        "    elif model_name == \"xgboost\":\n",
        "        file_path = destination.with_suffix(\".json\")\n",
        "        model.save_model(str(file_path))\n",
        "    elif model_name == \"tabnet\":\n",
        "        base_path = str(destination)\n",
        "        if base_path.endswith(\".zip\"):\n",
        "            base_path = base_path[:-4]\n",
        "        model.save_model(base_path)\n",
        "        file_path = Path(f\"{base_path}.zip\")\n",
        "    else:\n",
        "        file_path = destination.with_suffix(\".pkl\")\n",
        "        with open(file_path, \"wb\") as fp:\n",
        "            pickle.dump(model, fp)\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def lightgbm_predict_proba(model: lgb.Booster, data):\n",
        "    best_iter = model.best_iteration or model.current_iteration()\n",
        "    return model.predict(data, num_iteration=best_iter)\n",
        "\n",
        "\n",
        "def compute_fold_sample_weight(y_values):\n",
        "    return compute_sample_weight(class_weight=\"balanced\", y=y_values)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.276101Z",
          "iopub.execute_input": "2025-10-05T11:25:52.276543Z",
          "iopub.status.idle": "2025-10-05T11:25:52.283502Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.276525Z",
          "shell.execute_reply": "2025-10-05T11:25:52.282878Z"
        },
        "id": "Rm03QxzrboeB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_catboost(X_train, X_val, y_train, y_val, params, fold, sample_weight=None):\n",
        "    model_params = {**params}\n",
        "    model_params.setdefault(\"loss_function\", \"MultiClass\")\n",
        "    model_params.setdefault(\"eval_metric\", \"MultiClass\")\n",
        "    if \"task_type\" not in model_params:\n",
        "        model_params[\"task_type\"] = \"CPU\"\n",
        "    if model_params.get(\"task_type\") == \"GPU\" and \"devices\" not in model_params:\n",
        "        model_params[\"devices\"] = \"0\"\n",
        "    model_params.setdefault(\"verbose\", 0)\n",
        "    model_params[\"random_state\"] = RANDOM_SEED + fold\n",
        "\n",
        "    train_pool = Pool(X_train, y_train, weight=sample_weight)\n",
        "    val_pool = Pool(X_val, y_val)\n",
        "\n",
        "    model = CatBoostClassifier(**model_params)\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=val_pool,\n",
        "        early_stopping_rounds=20,\n",
        "        use_best_model=True,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    val_proba = model.predict_proba(X_val)\n",
        "    val_preds = np.argmax(val_proba, axis=1)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "\n",
        "    return model, val_proba, accuracy\n"
      ],
      "metadata": {
        "id": "CaBGXOupeCap",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.284509Z",
          "iopub.execute_input": "2025-10-05T11:25:52.284765Z",
          "iopub.status.idle": "2025-10-05T11:25:52.305424Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.284743Z",
          "shell.execute_reply": "2025-10-05T11:25:52.30488Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lgb(X_train, X_val, y_train, y_val, params, fold, sample_weight=None, num_classes=3):\n",
        "    model_params = {**params}\n",
        "    model_params.setdefault(\"objective\", \"multiclass\")\n",
        "    model_params.setdefault(\"num_class\", num_classes)\n",
        "    model_params.setdefault(\"metric\", \"multi_logloss\")\n",
        "    model_params.setdefault(\"verbose\", -1)\n",
        "    model_params[\"seed\"] = RANDOM_SEED + fold\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weight)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        model_params,\n",
        "        train_data,\n",
        "        num_boost_round=200,\n",
        "        valid_sets=[val_data],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(period=0)]\n",
        "    )\n",
        "\n",
        "    val_proba = lightgbm_predict_proba(model, X_val)\n",
        "    val_preds = np.argmax(val_proba, axis=1)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "\n",
        "    return model, val_proba, accuracy\n"
      ],
      "metadata": {
        "id": "wgFHm1ENk5G1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.306067Z",
          "iopub.execute_input": "2025-10-05T11:25:52.306259Z",
          "iopub.status.idle": "2025-10-05T11:25:52.324387Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.306243Z",
          "shell.execute_reply": "2025-10-05T11:25:52.32384Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgb(X_train, X_val, y_train, y_val, params, fold, sample_weight=None, num_classes=3):\n",
        "    model_params = {**params}\n",
        "    model_params.setdefault(\"objective\", \"multi:softprob\")\n",
        "    model_params.setdefault(\"num_class\", num_classes)\n",
        "    model_params.setdefault(\"eval_metric\", \"mlogloss\")\n",
        "    model_params.setdefault(\"verbosity\", 0)\n",
        "    model_params[\"random_state\"] = RANDOM_SEED + fold\n",
        "    model_params[\"use_label_encoder\"] = False\n",
        "    model_params[\"early_stopping_rounds\"] = 20\n",
        "\n",
        "    model = XGBClassifier(**model_params)\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        sample_weight=sample_weight,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    val_proba = model.predict_proba(X_val)\n",
        "    val_preds = np.argmax(val_proba, axis=1)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "\n",
        "    return model, val_proba, accuracy\n"
      ],
      "metadata": {
        "id": "0nKka_U8nvif",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.325097Z",
          "iopub.execute_input": "2025-10-05T11:25:52.325341Z",
          "iopub.status.idle": "2025-10-05T11:25:52.338577Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.325324Z",
          "shell.execute_reply": "2025-10-05T11:25:52.33804Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tabnet(X_train, X_val, y_train, y_val, params, fold, sample_weight=None):\n",
        "    model_params = params.copy()\n",
        "    lr = model_params.pop(\"lr\", 0.01)\n",
        "\n",
        "    model_params[\"optimizer_fn\"] = torch.optim.AdamW\n",
        "    model_params[\"optimizer_params\"] = dict(lr=lr)\n",
        "    model_params.setdefault(\"verbose\", 0)\n",
        "\n",
        "    model = TabNetClassifier(**model_params)\n",
        "\n",
        "    model.fit(\n",
        "        X_train.values,\n",
        "        y_train,\n",
        "        eval_set=[(X_val.values, y_val)],\n",
        "        eval_name=[\"valid\"],\n",
        "        eval_metric=[\"logloss\"],\n",
        "        max_epochs=200,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False,\n",
        "        weights=sample_weight,\n",
        "        from_unsupervised=None,\n",
        "    )\n",
        "\n",
        "    val_proba = model.predict_proba(X_val.values)\n",
        "    val_pred = np.argmax(val_proba, axis=1)\n",
        "    accuracy = accuracy_score(y_val, val_pred)\n",
        "\n",
        "    return model, val_proba, accuracy\n"
      ],
      "metadata": {
        "id": "OPNchn4MSNsB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.340585Z",
          "iopub.execute_input": "2025-10-05T11:25:52.341004Z",
          "iopub.status.idle": "2025-10-05T11:25:52.354702Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.340976Z",
          "shell.execute_reply": "2025-10-05T11:25:52.354132Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_catboost(trial, X, y, n_splits=5, random_seed=RANDOM_SEED):\n",
        "    params = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
        "        \"task_type\": trial.suggest_categorical(\"task_type\", [\"CPU\", \"GPU\"]),\n",
        "        \"loss_function\": \"MultiClass\",\n",
        "        \"eval_metric\": \"MultiClass\",\n",
        "        \"random_seed\": random_seed,\n",
        "        \"verbose\": 0,\n",
        "    }\n",
        "    if params[\"task_type\"] == \"GPU\":\n",
        "        params[\"devices\"] = \"0\"\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_weights = compute_fold_sample_weight(y_train)\n",
        "\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            sample_weight=train_weights,\n",
        "            eval_set=(X_val, y_val),\n",
        "            early_stopping_rounds=20,\n",
        "            verbose=False,\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        accuracies.append(accuracy_score(y_val, preds))\n",
        "\n",
        "    return float(np.mean(accuracies))\n"
      ],
      "metadata": {
        "id": "6L98bGoWC1BF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.355346Z",
          "iopub.execute_input": "2025-10-05T11:25:52.355637Z",
          "iopub.status.idle": "2025-10-05T11:25:52.368291Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.355619Z",
          "shell.execute_reply": "2025-10-05T11:25:52.367648Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_lgb(trial, X, y, num_classes, n_splits=5, random_seed=RANDOM_SEED):\n",
        "    params = {\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"num_class\": num_classes,\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"verbose\": -1,\n",
        "        \"seed\": random_seed,\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_weights = compute_fold_sample_weight(y_train)\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weights)\n",
        "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[val_data],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(period=0)]\n",
        "        )\n",
        "\n",
        "        preds = np.argmax(lightgbm_predict_proba(model, X_val), axis=1)\n",
        "        accuracies.append(accuracy_score(y_val, preds))\n",
        "\n",
        "    return float(np.mean(accuracies))\n"
      ],
      "metadata": {
        "id": "loxVLr4yDcfu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.36886Z",
          "iopub.execute_input": "2025-10-05T11:25:52.369448Z",
          "iopub.status.idle": "2025-10-05T11:25:52.385377Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.369425Z",
          "shell.execute_reply": "2025-10-05T11:25:52.384633Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgb(trial, X, y, num_classes, n_splits=5, random_seed=RANDOM_SEED):\n",
        "    params = {\n",
        "        \"objective\": \"multi:softprob\",\n",
        "        \"num_class\": num_classes,\n",
        "        \"eval_metric\": \"mlogloss\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "        \"random_state\": random_seed,\n",
        "        \"verbosity\": 0,\n",
        "        \"use_label_encoder\": False,\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_weights = compute_fold_sample_weight(y_train)\n",
        "\n",
        "        model = XGBClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            sample_weight=train_weights,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            early_stopping_rounds=20,\n",
        "            verbose=False,\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        accuracies.append(accuracy_score(y_val, preds))\n",
        "\n",
        "    return float(np.mean(accuracies))\n"
      ],
      "metadata": {
        "id": "-s19NrbWDhuZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.386032Z",
          "iopub.execute_input": "2025-10-05T11:25:52.386203Z",
          "iopub.status.idle": "2025-10-05T11:25:52.398702Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.38619Z",
          "shell.execute_reply": "2025-10-05T11:25:52.398047Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_tabnet(trial, X, y, n_splits=5, random_seed=42):\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer_fn\", [\"adam\", \"adamw\"])\n",
        "\n",
        "    optimizer_map = {\n",
        "        \"adam\": torch.optim.Adam,\n",
        "        \"adamw\": torch.optim.AdamW,\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64, step=8),\n",
        "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64, step=8),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.1),\n",
        "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
        "        \"optimizer_fn\": optimizer_map[optimizer_name],\n",
        "        \"optimizer_params\": {\"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)},\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
        "        \"seed\": random_seed,\n",
        "        \"verbose\": 0,\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_weights = compute_fold_sample_weight(y_train)\n",
        "\n",
        "        model = TabNetClassifier(**params)\n",
        "\n",
        "        model.fit(\n",
        "            X_train.values,\n",
        "            y_train.values,\n",
        "            eval_set=[(X_val.values, y_val.values)],\n",
        "            eval_metric=[\"accuracy\"],\n",
        "            max_epochs=200,\n",
        "            patience=2,\n",
        "            batch_size=1024,\n",
        "            virtual_batch_size=128,\n",
        "            weights=train_weights,\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val.values)\n",
        "        accuracies.append(accuracy_score(y_val, preds))\n",
        "\n",
        "    return float(np.mean(accuracies))\n"
      ],
      "metadata": {
        "id": "lZPh7e2RU0Ji",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.399298Z",
          "iopub.execute_input": "2025-10-05T11:25:52.399568Z",
          "iopub.status.idle": "2025-10-05T11:25:52.416262Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.399547Z",
          "shell.execute_reply": "2025-10-05T11:25:52.415676Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_stacking_pipeline(\n",
        "    X,\n",
        "    y,\n",
        "    *,\n",
        "    test_size=0.2,\n",
        "    n_trials=30,\n",
        "    base_cv_splits=10,\n",
        "    optuna_cv_splits=5,\n",
        "    artifact_root: Path = ARTIFACT_ROOT,\n",
        "    random_seed: int = RANDOM_SEED,\n",
        "):\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X_df = pd.DataFrame(X)\n",
        "    else:\n",
        "        X_df = X.copy()\n",
        "\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        if y.shape[1] != 1:\n",
        "            raise ValueError(\"y must be a 1-D vector.\")\n",
        "        y_series = y.iloc[:, 0].copy()\n",
        "    elif isinstance(y, pd.Series):\n",
        "        y_series = y.copy()\n",
        "    else:\n",
        "        y_series = pd.Series(y)\n",
        "\n",
        "    if X_df.shape[0] != len(y_series):\n",
        "        raise ValueError(\"X and y must have the same number of rows.\")\n",
        "    if len(np.unique(y_series)) < 2:\n",
        "        raise ValueError(\"y must contain at least two classes.\")\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "    artifact_root = ensure_dir(Path(artifact_root))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_df,\n",
        "        y_series,\n",
        "        test_size=test_size,\n",
        "        stratify=y_series,\n",
        "        random_state=random_seed,\n",
        "    )\n",
        "\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    objective_wrappers = {\n",
        "        \"catboost\": partial(\n",
        "            objective_catboost,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            n_splits=optuna_cv_splits,\n",
        "            random_seed=random_seed,\n",
        "        ),\n",
        "        \"lightgbm\": partial(\n",
        "            objective_lgb,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            num_classes=num_classes,\n",
        "            n_splits=optuna_cv_splits,\n",
        "            random_seed=random_seed,\n",
        "        ),\n",
        "        \"xgboost\": partial(\n",
        "            objective_xgb,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            num_classes=num_classes,\n",
        "            n_splits=optuna_cv_splits,\n",
        "            random_seed=random_seed,\n",
        "        ),\n",
        "        \"tabnet\": partial(\n",
        "            objective_tabnet,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            n_splits=optuna_cv_splits,\n",
        "            random_seed=random_seed,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    best_params = {}\n",
        "    for model_name, objective_fn in objective_wrappers.items():\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective_fn, n_trials=n_trials)\n",
        "        best_params[model_name] = study.best_params\n",
        "        print(f\"{model_name.title()} best params: {study.best_params}\")\n",
        "\n",
        "    with open(artifact_root / \"best_params.json\", \"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(best_params, fp, indent=2)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=base_cv_splits, shuffle=True, random_state=random_seed)\n",
        "\n",
        "    oof_predictions = {\n",
        "        \"catboost\": np.zeros((len(y_train), num_classes)),\n",
        "        \"lightgbm\": np.zeros((len(y_train), num_classes)),\n",
        "        \"xgboost\": np.zeros((len(y_train), num_classes)),\n",
        "        \"tabnet\": np.zeros((len(y_train), num_classes)),\n",
        "    }\n",
        "    test_predictions = {\n",
        "        \"catboost\": np.zeros((len(y_test), num_classes)),\n",
        "        \"lightgbm\": np.zeros((len(y_test), num_classes)),\n",
        "        \"xgboost\": np.zeros((len(y_test), num_classes)),\n",
        "        \"tabnet\": np.zeros((len(y_test), num_classes)),\n",
        "    }\n",
        "    fold_accuracies = {model: [] for model in oof_predictions}\n",
        "    saved_model_paths = {model: [] for model in oof_predictions}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        train_weights = compute_fold_sample_weight(y_tr)\n",
        "\n",
        "        cb_model, cb_val_proba, cb_acc = train_catboost(\n",
        "            X_tr,\n",
        "            X_val,\n",
        "            y_tr,\n",
        "            y_val,\n",
        "            params=best_params[\"catboost\"],\n",
        "            fold=fold,\n",
        "            sample_weight=train_weights,\n",
        "        )\n",
        "        oof_predictions[\"catboost\"][val_idx] = cb_val_proba\n",
        "        test_predictions[\"catboost\"] += cb_model.predict_proba(X_test)\n",
        "        fold_accuracies[\"catboost\"].append(cb_acc)\n",
        "        saved_model_paths[\"catboost\"].append(\n",
        "            str(persist_model(cb_model, artifact_root / \"catboost\" / f\"fold_{fold}\", \"catboost\"))\n",
        "        )\n",
        "\n",
        "        lgb_model, lgb_val_proba, lgb_acc = train_lgb(\n",
        "            X_tr,\n",
        "            X_val,\n",
        "            y_tr,\n",
        "            y_val,\n",
        "            params=best_params[\"lightgbm\"],\n",
        "            fold=fold,\n",
        "            sample_weight=train_weights,\n",
        "            num_classes=num_classes,\n",
        "        )\n",
        "        oof_predictions[\"lightgbm\"][val_idx] = lgb_val_proba\n",
        "        test_predictions[\"lightgbm\"] += lightgbm_predict_proba(lgb_model, X_test)\n",
        "        fold_accuracies[\"lightgbm\"].append(lgb_acc)\n",
        "        saved_model_paths[\"lightgbm\"].append(\n",
        "            str(persist_model(lgb_model, artifact_root / \"lightgbm\" / f\"fold_{fold}\", \"lightgbm\"))\n",
        "        )\n",
        "\n",
        "        xgb_model, xgb_val_proba, xgb_acc = train_xgb(\n",
        "            X_tr,\n",
        "            X_val,\n",
        "            y_tr,\n",
        "            y_val,\n",
        "            params=best_params[\"xgboost\"],\n",
        "            fold=fold,\n",
        "            sample_weight=train_weights,\n",
        "            num_classes=num_classes,\n",
        "        )\n",
        "        oof_predictions[\"xgboost\"][val_idx] = xgb_val_proba\n",
        "        test_predictions[\"xgboost\"] += xgb_model.predict_proba(X_test)\n",
        "        fold_accuracies[\"xgboost\"].append(xgb_acc)\n",
        "        saved_model_paths[\"xgboost\"].append(\n",
        "            str(persist_model(xgb_model, artifact_root / \"xgboost\" / f\"fold_{fold}\", \"xgboost\"))\n",
        "        )\n",
        "\n",
        "        tabnet_model, tabnet_val_proba, tabnet_acc = train_tabnet(\n",
        "            X_tr,\n",
        "            X_val,\n",
        "            y_tr,\n",
        "            y_val,\n",
        "            params=best_params[\"tabnet\"],\n",
        "            fold=fold,\n",
        "            sample_weight=train_weights,\n",
        "        )\n",
        "        oof_predictions[\"tabnet\"][val_idx] = tabnet_val_proba\n",
        "        test_predictions[\"tabnet\"] += tabnet_model.predict_proba(X_test.values)\n",
        "        fold_accuracies[\"tabnet\"].append(tabnet_acc)\n",
        "        saved_model_paths[\"tabnet\"].append(\n",
        "            str(persist_model(tabnet_model, artifact_root / \"tabnet\" / f\"fold_{fold}\", \"tabnet\"))\n",
        "        )\n",
        "\n",
        "        print(f\"Fold {fold} complete.\")\n",
        "\n",
        "    for model_name in test_predictions:\n",
        "        test_predictions[model_name] /= base_cv_splits\n",
        "\n",
        "    meta_features_train = np.hstack([reshape_for_meta(oof_predictions[name]) for name in oof_predictions])\n",
        "    meta_features_test = np.hstack([reshape_for_meta(test_predictions[name]) for name in test_predictions])\n",
        "\n",
        "    meta_model = LogisticRegressionCV(\n",
        "        cv=base_cv_splits,\n",
        "        multi_class=\"multinomial\",\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=random_seed,\n",
        "    )\n",
        "    meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "    meta_train_preds = meta_model.predict(meta_features_train)\n",
        "    meta_train_acc = accuracy_score(y_train, meta_train_preds)\n",
        "\n",
        "    meta_test_preds = meta_model.predict(meta_features_test)\n",
        "    meta_test_acc = accuracy_score(y_test, meta_test_preds)\n",
        "    meta_test_proba = meta_model.predict_proba(meta_features_test)\n",
        "    meta_report = classification_report(y_test, meta_test_preds, digits=4)\n",
        "\n",
        "    meta_model_path = artifact_root / \"meta_model.joblib\"\n",
        "    joblib.dump(meta_model, meta_model_path)\n",
        "\n",
        "    predictions_df = pd.DataFrame(\n",
        "        {\n",
        "            \"y_true\": y_test.values,\n",
        "            \"y_pred\": meta_test_preds,\n",
        "        }\n",
        "    )\n",
        "    predictions_path = artifact_root / \"meta_model_test_predictions.csv\"\n",
        "    predictions_df.to_csv(predictions_path, index=False)\n",
        "\n",
        "    np.save(artifact_root / \"meta_features_train.npy\", meta_features_train)\n",
        "    np.save(artifact_root / \"meta_features_test.npy\", meta_features_test)\n",
        "    np.save(artifact_root / \"meta_test_proba.npy\", meta_test_proba)\n",
        "\n",
        "    summary = {\n",
        "        \"best_params\": best_params,\n",
        "        \"fold_metrics\": {\n",
        "            name: {\n",
        "                \"mean_accuracy\": float(np.mean(scores)),\n",
        "                \"std_accuracy\": float(np.std(scores)),\n",
        "            }\n",
        "            for name, scores in fold_accuracies.items()\n",
        "        },\n",
        "        \"meta_train_accuracy\": float(meta_train_acc),\n",
        "        \"meta_test_accuracy\": float(meta_test_acc),\n",
        "        \"classification_report\": meta_report,\n",
        "        \"saved_models\": saved_model_paths,\n",
        "        \"meta_model_path\": str(meta_model_path),\n",
        "        \"test_predictions_path\": str(predictions_path),\n",
        "    }\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "VsMKzppbpKTH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.417051Z",
          "iopub.execute_input": "2025-10-05T11:25:52.417292Z",
          "iopub.status.idle": "2025-10-05T11:25:52.438567Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.417272Z",
          "shell.execute_reply": "2025-10-05T11:25:52.43797Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path(\"/kaggle/input/nasa-dataset-tryhard/combined_imputed_values.csv\")\n",
        "\n",
        "if data_path.exists():\n",
        "    df = pd.read_csv(data_path)\n",
        "    if \"disposition\" not in df.columns:\n",
        "        raise KeyError(\"Column 'disposition' is missing from the dataset.\")\n",
        "    X = df.drop(columns=[\"disposition\"])\n",
        "    y = df[\"disposition\"]\n",
        "    print(f\"Loaded dataset with shape {df.shape}.\")\n",
        "else:\n",
        "    print(\"Dataset 'combined_imputed_values.csv' not found. Define X and y manually.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.43931Z",
          "iopub.execute_input": "2025-10-05T11:25:52.439545Z",
          "iopub.status.idle": "2025-10-05T11:25:52.53572Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.439524Z",
          "shell.execute_reply": "2025-10-05T11:25:52.535142Z"
        },
        "id": "nPrcEG03boeF",
        "outputId": "3077adf8-9a3d-4073-e9f0-d2d003cb6f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loaded dataset with shape (16707, 11).\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if \"X\" in globals() and \"y\" in globals():\n",
        "    le = LabelEncoder()\n",
        "    y_fixed = le.fit_transform(y)\n",
        "\n",
        "    pipeline_results = run_stacking_pipeline(\n",
        "        X,\n",
        "        y_fixed,\n",
        "        test_size=0.2,\n",
        "        n_trials=30,\n",
        "        base_cv_splits=10,\n",
        "        optuna_cv_splits=5,\n",
        "        artifact_root=ARTIFACT_ROOT,\n",
        "        random_seed=RANDOM_SEED,\n",
        "    )\n",
        "    pipeline_results\n",
        "else:\n",
        "    print(\"Define X (features) and y (target) before running the stacking pipeline.\")\n"
      ],
      "metadata": {
        "id": "OGCwUqMjuQys",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T11:25:52.536334Z",
          "iopub.execute_input": "2025-10-05T11:25:52.536572Z",
          "iopub.status.idle": "2025-10-05T12:29:52.969397Z",
          "shell.execute_reply.started": "2025-10-05T11:25:52.536546Z",
          "shell.execute_reply": "2025-10-05T12:29:52.968787Z"
        },
        "outputId": "17f02f64-4880-4837-a47b-ea9c4a628148"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:25:52,549] A new study created in memory with name: no-name-f0de4d7e-e51e-4f1d-aba7-2c668f68e483\n[I 2025-10-05 11:26:48,239] Trial 0 finished with value: 0.6614291058735503 and parameters: {'iterations': 217, 'depth': 10, 'learning_rate': 0.0016083788090401577, 'l2_leaf_reg': 6.971160211362632, 'task_type': 'CPU'}. Best is trial 0 with value: 0.6614291058735503.\n[I 2025-10-05 11:27:09,785] Trial 1 finished with value: 0.7014590347923682 and parameters: {'iterations': 177, 'depth': 4, 'learning_rate': 0.14844655685226354, 'l2_leaf_reg': 3.127496162207505, 'task_type': 'GPU'}. Best is trial 1 with value: 0.7014590347923682.\n[I 2025-10-05 11:27:13,512] Trial 2 finished with value: 0.6439955106621774 and parameters: {'iterations': 151, 'depth': 5, 'learning_rate': 0.0011846124990474549, 'l2_leaf_reg': 2.5244599550898803, 'task_type': 'CPU'}. Best is trial 1 with value: 0.7014590347923682.\n[I 2025-10-05 11:27:19,956] Trial 3 finished with value: 0.6885147774036664 and parameters: {'iterations': 155, 'depth': 6, 'learning_rate': 0.04048907976547664, 'l2_leaf_reg': 5.6821048474642275, 'task_type': 'GPU'}. Best is trial 1 with value: 0.7014590347923682.\n[I 2025-10-05 11:27:28,232] Trial 4 finished with value: 0.672053872053872 and parameters: {'iterations': 274, 'depth': 5, 'learning_rate': 0.015169334701035063, 'l2_leaf_reg': 2.9212122214022127, 'task_type': 'GPU'}. Best is trial 1 with value: 0.7014590347923682.\n[I 2025-10-05 11:27:34,371] Trial 5 finished with value: 0.7077441077441078 and parameters: {'iterations': 123, 'depth': 6, 'learning_rate': 0.13600929612828594, 'l2_leaf_reg': 3.896940785109739, 'task_type': 'GPU'}. Best is trial 5 with value: 0.7077441077441078.\n[I 2025-10-05 11:27:39,857] Trial 6 finished with value: 0.6407033295922184 and parameters: {'iterations': 119, 'depth': 5, 'learning_rate': 0.0028877859207319455, 'l2_leaf_reg': 4.458100227356391, 'task_type': 'GPU'}. Best is trial 5 with value: 0.7077441077441078.\n[I 2025-10-05 11:27:53,950] Trial 7 finished with value: 0.6582117471006359 and parameters: {'iterations': 263, 'depth': 8, 'learning_rate': 0.0010993773774040942, 'l2_leaf_reg': 2.2688111303132747, 'task_type': 'GPU'}. Best is trial 5 with value: 0.7077441077441078.\n[I 2025-10-05 11:28:05,083] Trial 8 finished with value: 0.6962962962962963 and parameters: {'iterations': 339, 'depth': 6, 'learning_rate': 0.02677207089079974, 'l2_leaf_reg': 6.904320357388759, 'task_type': 'GPU'}. Best is trial 5 with value: 0.7077441077441078.\n[I 2025-10-05 11:30:05,003] Trial 9 finished with value: 0.6829031051253274 and parameters: {'iterations': 485, 'depth': 10, 'learning_rate': 0.0069042025529372514, 'l2_leaf_reg': 7.6857945650317205, 'task_type': 'CPU'}. Best is trial 5 with value: 0.7077441077441078.\n[I 2025-10-05 11:30:19,069] Trial 10 finished with value: 0.71440329218107 and parameters: {'iterations': 361, 'depth': 8, 'learning_rate': 0.1708931523305348, 'l2_leaf_reg': 1.0230306202512498, 'task_type': 'CPU'}. Best is trial 10 with value: 0.71440329218107.\n[I 2025-10-05 11:30:28,979] Trial 11 finished with value: 0.7129068462401796 and parameters: {'iterations': 385, 'depth': 8, 'learning_rate': 0.2585170513723628, 'l2_leaf_reg': 1.0090318475140607, 'task_type': 'CPU'}. Best is trial 10 with value: 0.71440329218107.\n[I 2025-10-05 11:30:37,984] Trial 12 finished with value: 0.712532734754957 and parameters: {'iterations': 382, 'depth': 8, 'learning_rate': 0.29926682713837827, 'l2_leaf_reg': 1.1152609075487507, 'task_type': 'CPU'}. Best is trial 10 with value: 0.71440329218107.\n[I 2025-10-05 11:31:10,599] Trial 13 finished with value: 0.7171717171717172 and parameters: {'iterations': 421, 'depth': 8, 'learning_rate': 0.06267339315230606, 'l2_leaf_reg': 9.69791575784518, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:32:16,252] Trial 14 finished with value: 0.7164983164983164 and parameters: {'iterations': 490, 'depth': 9, 'learning_rate': 0.0670802733775792, 'l2_leaf_reg': 9.870581117679812, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:33:23,653] Trial 15 finished with value: 0.714552936775159 and parameters: {'iterations': 498, 'depth': 9, 'learning_rate': 0.060208049264554656, 'l2_leaf_reg': 9.998211984693542, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:34:19,773] Trial 16 finished with value: 0.7159745604190049 and parameters: {'iterations': 422, 'depth': 9, 'learning_rate': 0.06829190877647177, 'l2_leaf_reg': 9.9356563686857, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:35:20,582] Trial 17 finished with value: 0.6923307145529367 and parameters: {'iterations': 450, 'depth': 9, 'learning_rate': 0.01310531956128313, 'l2_leaf_reg': 8.622819861861768, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:35:40,609] Trial 18 finished with value: 0.7154508043396932 and parameters: {'iterations': 438, 'depth': 7, 'learning_rate': 0.08296847114191173, 'l2_leaf_reg': 8.639237394405896, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:35:55,197] Trial 19 finished with value: 0.6953236064347176 and parameters: {'iterations': 317, 'depth': 7, 'learning_rate': 0.02873144587990344, 'l2_leaf_reg': 8.754296611733155, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:37:23,804] Trial 20 finished with value: 0.7150766928544707 and parameters: {'iterations': 414, 'depth': 10, 'learning_rate': 0.09108322360818631, 'l2_leaf_reg': 9.309490491441428, 'task_type': 'CPU'}. Best is trial 13 with value: 0.7171717171717172.\n[I 2025-10-05 11:38:24,282] Trial 21 finished with value: 0.7182940516273849 and parameters: {'iterations': 449, 'depth': 9, 'learning_rate': 0.07452489162580808, 'l2_leaf_reg': 9.624071790339087, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:39:27,175] Trial 22 finished with value: 0.7143284698840254 and parameters: {'iterations': 469, 'depth': 9, 'learning_rate': 0.04440245096544866, 'l2_leaf_reg': 7.955626382219206, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:40:21,907] Trial 23 finished with value: 0.6774410774410774 and parameters: {'iterations': 407, 'depth': 9, 'learning_rate': 0.007712933245701016, 'l2_leaf_reg': 9.333095075460639, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:40:57,412] Trial 24 finished with value: 0.7051253273475495 and parameters: {'iterations': 459, 'depth': 8, 'learning_rate': 0.024972487852236606, 'l2_leaf_reg': 5.9716665626048595, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:42:29,939] Trial 25 finished with value: 0.7172465394687617 and parameters: {'iterations': 482, 'depth': 10, 'learning_rate': 0.09504457964279647, 'l2_leaf_reg': 7.834173816794635, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:43:48,775] Trial 26 finished with value: 0.7160493827160493 and parameters: {'iterations': 389, 'depth': 10, 'learning_rate': 0.11381417888626802, 'l2_leaf_reg': 7.775430696099475, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:45:38,221] Trial 27 finished with value: 0.7133557800224467 and parameters: {'iterations': 446, 'depth': 10, 'learning_rate': 0.045094212276032476, 'l2_leaf_reg': 6.745934758411898, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:45:49,482] Trial 28 finished with value: 0.7138047138047139 and parameters: {'iterations': 342, 'depth': 7, 'learning_rate': 0.21311489072039577, 'l2_leaf_reg': 8.947343602200466, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:47:34,088] Trial 29 finished with value: 0.6888140665918444 and parameters: {'iterations': 427, 'depth': 10, 'learning_rate': 0.010590118067339059, 'l2_leaf_reg': 7.979993880713222, 'task_type': 'CPU'}. Best is trial 21 with value: 0.7182940516273849.\n[I 2025-10-05 11:47:34,089] A new study created in memory with name: no-name-d5b37050-74d4-4641-82e9-ce3f5f45086c\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Catboost best params: {'iterations': 449, 'depth': 9, 'learning_rate': 0.07452489162580808, 'l2_leaf_reg': 9.624071790339087, 'task_type': 'CPU'}\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.978074\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.981645\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.97487\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.975656\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:35,998] Trial 0 finished with value: 0.6674148896371118 and parameters: {'learning_rate': 0.0027897994056749476, 'num_leaves': 128, 'max_depth': 5, 'min_child_samples': 24, 'subsample': 0.7534157338565933, 'colsample_bytree': 0.6798808676116319}. Best is trial 0 with value: 0.6674148896371118.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.980139\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.843352\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.851625\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.83698\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.837279\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:40,210] Trial 1 finished with value: 0.7000374111485222 and parameters: {'learning_rate': 0.00671633835823325, 'num_leaves': 81, 'max_depth': -1, 'min_child_samples': 44, 'subsample': 0.6270064392231525, 'colsample_bytree': 0.5554441104106529}. Best is trial 1 with value: 0.7000374111485222.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.846885\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.749629\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.760657\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.732886\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.736959\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:41,942] Trial 2 finished with value: 0.6808829031051253 and parameters: {'learning_rate': 0.025488325617256665, 'num_leaves': 114, 'max_depth': 5, 'min_child_samples': 7, 'subsample': 0.9834581528025337, 'colsample_bytree': 0.501543952853927}. Best is trial 1 with value: 0.7000374111485222.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.756032\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.975667\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.97926\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.972948\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.973628\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:44,111] Trial 3 finished with value: 0.6754208754208755 and parameters: {'learning_rate': 0.0026624197337240534, 'num_leaves': 34, 'max_depth': 6, 'min_child_samples': 19, 'subsample': 0.7951570860953145, 'colsample_bytree': 0.6566042436943016}. Best is trial 1 with value: 0.7000374111485222.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.977921\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[40]\tvalid_0's multi_logloss: 0.667875\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[30]\tvalid_0's multi_logloss: 0.67575\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[43]\tvalid_0's multi_logloss: 0.636838\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[42]\tvalid_0's multi_logloss: 0.649591\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:45,733] Trial 4 finished with value: 0.7156752712308269 and parameters: {'learning_rate': 0.21751054058749017, 'num_leaves': 49, 'max_depth': 11, 'min_child_samples': 30, 'subsample': 0.8384781687570805, 'colsample_bytree': 0.572762313312746}. Best is trial 4 with value: 0.7156752712308269.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[49]\tvalid_0's multi_logloss: 0.667256\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.856082\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.862219\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.848528\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.849708\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:47,699] Trial 5 finished with value: 0.6719042274597831 and parameters: {'learning_rate': 0.007293546750763161, 'num_leaves': 65, 'max_depth': 5, 'min_child_samples': 11, 'subsample': 0.9790931893659285, 'colsample_bytree': 0.9635446679051796}. Best is trial 4 with value: 0.7156752712308269.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.858868\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.920433\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.925573\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.916386\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.917516\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:50,851] Trial 6 finished with value: 0.6823793490460157 and parameters: {'learning_rate': 0.004115274581197572, 'num_leaves': 81, 'max_depth': 7, 'min_child_samples': 34, 'subsample': 0.8989445791779906, 'colsample_bytree': 0.6910574828932841}. Best is trial 4 with value: 0.7156752712308269.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.9237\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[62]\tvalid_0's multi_logloss: 0.669823\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[46]\tvalid_0's multi_logloss: 0.675006\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[53]\tvalid_0's multi_logloss: 0.641071\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[70]\tvalid_0's multi_logloss: 0.640192\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:52,597] Trial 7 finished with value: 0.7173213617658061 and parameters: {'learning_rate': 0.2126998338160467, 'num_leaves': 39, 'max_depth': 12, 'min_child_samples': 14, 'subsample': 0.9951653568417376, 'colsample_bytree': 0.6532709551451061}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[42]\tvalid_0's multi_logloss: 0.673703\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[99]\tvalid_0's multi_logloss: 0.662969\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.677123\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.642133\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.644621\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:47:56,259] Trial 8 finished with value: 0.7135054246165358 and parameters: {'learning_rate': 0.040146102107475407, 'num_leaves': 66, 'max_depth': 10, 'min_child_samples': 30, 'subsample': 0.9856063624511953, 'colsample_bytree': 0.8182590864029704}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.672566\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.777339\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.785148\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.769032\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.768624\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:01,180] Trial 9 finished with value: 0.7049756827534606 and parameters: {'learning_rate': 0.009319973980585948, 'num_leaves': 77, 'max_depth': -1, 'min_child_samples': 7, 'subsample': 0.7928842683341906, 'colsample_bytree': 0.8692259614591007}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.780961\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[60]\tvalid_0's multi_logloss: 0.66992\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[93]\tvalid_0's multi_logloss: 0.676717\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[73]\tvalid_0's multi_logloss: 0.64456\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[91]\tvalid_0's multi_logloss: 0.647472\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:02,680] Trial 10 finished with value: 0.7131313131313132 and parameters: {'learning_rate': 0.22465217188764536, 'num_leaves': 19, 'max_depth': 14, 'min_child_samples': 16, 'subsample': 0.5233889479225261, 'colsample_bytree': 0.7928944554495616}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[61]\tvalid_0's multi_logloss: 0.673121\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[29]\tvalid_0's multi_logloss: 0.670292\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[34]\tvalid_0's multi_logloss: 0.680365\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[37]\tvalid_0's multi_logloss: 0.642089\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[55]\tvalid_0's multi_logloss: 0.646093\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:04,110] Trial 11 finished with value: 0.7116348671904227 and parameters: {'learning_rate': 0.26614818516171235, 'num_leaves': 43, 'max_depth': 12, 'min_child_samples': 38, 'subsample': 0.8750752480289165, 'colsample_bytree': 0.5910186511915644}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[35]\tvalid_0's multi_logloss: 0.665805\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[98]\tvalid_0's multi_logloss: 0.666402\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[99]\tvalid_0's multi_logloss: 0.672158\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.631796\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.641712\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:06,772] Trial 12 finished with value: 0.7130564908342686 and parameters: {'learning_rate': 0.08561081369042785, 'num_leaves': 45, 'max_depth': 10, 'min_child_samples': 24, 'subsample': 0.8817891139648931, 'colsample_bytree': 0.6063775495098235}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.661635\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.674439\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.68597\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.653612\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.655659\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:08,447] Trial 13 finished with value: 0.7099887766554432 and parameters: {'learning_rate': 0.07952065928666363, 'num_leaves': 17, 'max_depth': 15, 'min_child_samples': 48, 'subsample': 0.6130607246907399, 'colsample_bytree': 0.7387283797641221}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.675527\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 1.04367\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 1.04512\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 1.04266\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 1.04283\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:11,620] Trial 14 finished with value: 0.6877665544332212 and parameters: {'learning_rate': 0.001048960626852366, 'num_leaves': 55, 'max_depth': 10, 'min_child_samples': 16, 'subsample': 0.9132941868389239, 'colsample_bytree': 0.5339267784171254}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 1.04464\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[55]\tvalid_0's multi_logloss: 0.66217\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[51]\tvalid_0's multi_logloss: 0.675216\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[58]\tvalid_0's multi_logloss: 0.633496\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[61]\tvalid_0's multi_logloss: 0.638441\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:14,792] Trial 15 finished with value: 0.7160493827160493 and parameters: {'learning_rate': 0.1198003188220262, 'num_leaves': 97, 'max_depth': 12, 'min_child_samples': 29, 'subsample': 0.650380207875193, 'colsample_bytree': 0.6241343802588284}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[57]\tvalid_0's multi_logloss: 0.666825\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[70]\tvalid_0's multi_logloss: 0.657405\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[84]\tvalid_0's multi_logloss: 0.673501\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.632311\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[94]\tvalid_0's multi_logloss: 0.628834\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:19,269] Trial 16 finished with value: 0.7170968948746727 and parameters: {'learning_rate': 0.07205422162611128, 'num_leaves': 102, 'max_depth': 13, 'min_child_samples': 38, 'subsample': 0.6591417715967576, 'colsample_bytree': 0.6507508170740849}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[87]\tvalid_0's multi_logloss: 0.662724\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[99]\tvalid_0's multi_logloss: 0.670769\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.685798\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.646273\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.651341\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:22,874] Trial 17 finished with value: 0.7109614665170221 and parameters: {'learning_rate': 0.045290664965310014, 'num_leaves': 101, 'max_depth': 8, 'min_child_samples': 38, 'subsample': 0.6834807945564568, 'colsample_bytree': 0.7193942927373695}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.67511\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[28]\tvalid_0's multi_logloss: 0.66112\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[39]\tvalid_0's multi_logloss: 0.675672\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[51]\tvalid_0's multi_logloss: 0.63585\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[43]\tvalid_0's multi_logloss: 0.64006\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:25,619] Trial 18 finished with value: 0.7129816685372241 and parameters: {'learning_rate': 0.13559836362773287, 'num_leaves': 99, 'max_depth': 14, 'min_child_samples': 42, 'subsample': 0.5399556701269588, 'colsample_bytree': 0.7815836621286778}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[44]\tvalid_0's multi_logloss: 0.66357\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.713946\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.728204\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.701864\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.70293\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:28,117] Trial 19 finished with value: 0.704900860456416 and parameters: {'learning_rate': 0.019983704071889936, 'num_leaves': 29, 'max_depth': 13, 'min_child_samples': 49, 'subsample': 0.7090404460101057, 'colsample_bytree': 0.8863701265097166}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.724189\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.743779\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.753007\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.725253\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.726261\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.748314\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:29,139] Trial 20 finished with value: 0.6774410774410774 and parameters: {'learning_rate': 0.051101501464039266, 'num_leaves': 91, 'max_depth': 3, 'min_child_samples': 21, 'subsample': 0.5736562531127283, 'colsample_bytree': 0.6774527774670338}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[42]\tvalid_0's multi_logloss: 0.659096\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[41]\tvalid_0's multi_logloss: 0.674082\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[49]\tvalid_0's multi_logloss: 0.633253\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[55]\tvalid_0's multi_logloss: 0.645002\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:32,205] Trial 21 finished with value: 0.715001870557426 and parameters: {'learning_rate': 0.12578452250311167, 'num_leaves': 111, 'max_depth': 12, 'min_child_samples': 34, 'subsample': 0.6690457011915492, 'colsample_bytree': 0.6347834105508018}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[62]\tvalid_0's multi_logloss: 0.662554\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[40]\tvalid_0's multi_logloss: 0.659268\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[39]\tvalid_0's multi_logloss: 0.682333\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[49]\tvalid_0's multi_logloss: 0.630215\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[45]\tvalid_0's multi_logloss: 0.635602\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:34,994] Trial 22 finished with value: 0.7161990273101384 and parameters: {'learning_rate': 0.1364590345716137, 'num_leaves': 91, 'max_depth': 15, 'min_child_samples': 27, 'subsample': 0.6201799698287238, 'colsample_bytree': 0.6159937208393965}. Best is trial 7 with value: 0.7173213617658061.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[54]\tvalid_0's multi_logloss: 0.663372\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[33]\tvalid_0's multi_logloss: 0.66203\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[38]\tvalid_0's multi_logloss: 0.685276\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[32]\tvalid_0's multi_logloss: 0.641802\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[35]\tvalid_0's multi_logloss: 0.645192\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:37,895] Trial 23 finished with value: 0.7175458286569398 and parameters: {'learning_rate': 0.1656477472261641, 'num_leaves': 112, 'max_depth': 15, 'min_child_samples': 13, 'subsample': 0.5848078151470397, 'colsample_bytree': 0.7191904741720603}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[37]\tvalid_0's multi_logloss: 0.668938\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[19]\tvalid_0's multi_logloss: 0.668483\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[15]\tvalid_0's multi_logloss: 0.692649\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[17]\tvalid_0's multi_logloss: 0.651585\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[22]\tvalid_0's multi_logloss: 0.648215\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:39,969] Trial 24 finished with value: 0.7123082678638234 and parameters: {'learning_rate': 0.28731575281534805, 'num_leaves': 117, 'max_depth': 14, 'min_child_samples': 13, 'subsample': 0.5777867208639923, 'colsample_bytree': 0.7174413640823359}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[17]\tvalid_0's multi_logloss: 0.680123\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[67]\tvalid_0's multi_logloss: 0.666486\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[89]\tvalid_0's multi_logloss: 0.676368\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[87]\tvalid_0's multi_logloss: 0.631038\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.640317\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:45,049] Trial 25 finished with value: 0.7161242050130939 and parameters: {'learning_rate': 0.07196361729795456, 'num_leaves': 119, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.7302415897764825, 'colsample_bytree': 0.7634153530039292}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[87]\tvalid_0's multi_logloss: 0.66661\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[25]\tvalid_0's multi_logloss: 0.662538\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[27]\tvalid_0's multi_logloss: 0.679934\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[32]\tvalid_0's multi_logloss: 0.635592\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[24]\tvalid_0's multi_logloss: 0.64202\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:47,865] Trial 26 finished with value: 0.7156752712308269 and parameters: {'learning_rate': 0.17447651678499115, 'num_leaves': 126, 'max_depth': 15, 'min_child_samples': 10, 'subsample': 0.5693302285115125, 'colsample_bytree': 0.8213313946736842}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Early stopping, best iteration is:\n[29]\tvalid_0's multi_logloss: 0.663768\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.663056\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.682218\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.646571\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.651192\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:53,249] Trial 27 finished with value: 0.7161242050130939 and parameters: {'learning_rate': 0.02930895600020883, 'num_leaves': 107, 'max_depth': 13, 'min_child_samples': 16, 'subsample': 0.6968860778854251, 'colsample_bytree': 0.6563112335540406}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.674251\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.725353\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.737951\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.715064\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.714734\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:48:58,038] Trial 28 finished with value: 0.7095398428731763 and parameters: {'learning_rate': 0.014447968563766298, 'num_leaves': 88, 'max_depth': 13, 'min_child_samples': 20, 'subsample': 0.7720901855489246, 'colsample_bytree': 0.7104304348976098}. Best is trial 23 with value: 0.7175458286569398.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[100]\tvalid_0's multi_logloss: 0.731492\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[80]\tvalid_0's multi_logloss: 0.656204\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[80]\tvalid_0's multi_logloss: 0.672636\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[98]\tvalid_0's multi_logloss: 0.633166\nTraining until validation scores don't improve for 20 rounds\nDid not meet early stopping. Best iteration is:\n[95]\tvalid_0's multi_logloss: 0.632534\nTraining until validation scores don't improve for 20 rounds\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[I 2025-10-05 11:49:03,139] Trial 29 finished with value: 0.716273849607183 and parameters: {'learning_rate': 0.06515194241057828, 'num_leaves': 123, 'max_depth': 11, 'min_child_samples': 25, 'subsample': 0.7417786602239076, 'colsample_bytree': 0.6641592525030026}. Best is trial 23 with value: 0.7175458286569398.\n[I 2025-10-05 11:49:03,140] A new study created in memory with name: no-name-819de572-c779-4cdb-b1e3-79534012d8b5\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Did not meet early stopping. Best iteration is:\n[95]\tvalid_0's multi_logloss: 0.661779\nLightgbm best params: {'learning_rate': 0.1656477472261641, 'num_leaves': 112, 'max_depth': 15, 'min_child_samples': 13, 'subsample': 0.5848078151470397, 'colsample_bytree': 0.7191904741720603}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:05,665] Trial 0 finished with value: 0.7099139543583989 and parameters: {'learning_rate': 0.20873187647326455, 'max_depth': 4, 'subsample': 0.7052445393072131, 'colsample_bytree': 0.5857242716948167, 'n_estimators': 160}. Best is trial 0 with value: 0.7099139543583989.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:11,019] Trial 1 finished with value: 0.6870183314627759 and parameters: {'learning_rate': 0.011790061409492326, 'max_depth': 6, 'subsample': 0.9441352917703139, 'colsample_bytree': 0.5820299676514458, 'n_estimators': 204}. Best is trial 0 with value: 0.7099139543583989.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:16,265] Trial 2 finished with value: 0.7183688739244295 and parameters: {'learning_rate': 0.13505933035940546, 'max_depth': 9, 'subsample': 0.7695805373832778, 'colsample_bytree': 0.8543499556504438, 'n_estimators': 204}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:32,503] Trial 3 finished with value: 0.7150018705574261 and parameters: {'learning_rate': 0.021686117582799376, 'max_depth': 9, 'subsample': 0.8720181993545741, 'colsample_bytree': 0.5275807118573723, 'n_estimators': 299}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:37,526] Trial 4 finished with value: 0.6913580246913581 and parameters: {'learning_rate': 0.017544588473706817, 'max_depth': 6, 'subsample': 0.665525399412255, 'colsample_bytree': 0.5689071255956295, 'n_estimators': 192}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:45,245] Trial 5 finished with value: 0.7105125327347549 and parameters: {'learning_rate': 0.026341313857713263, 'max_depth': 6, 'subsample': 0.5787633668172183, 'colsample_bytree': 0.6479942067120326, 'n_estimators': 287}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:49,037] Trial 6 finished with value: 0.7066965955854845 and parameters: {'learning_rate': 0.04066463207982242, 'max_depth': 8, 'subsample': 0.6878454230608445, 'colsample_bytree': 0.8141879490779176, 'n_estimators': 69}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:49:53,343] Trial 7 finished with value: 0.7156004489337822 and parameters: {'learning_rate': 0.15761197309348135, 'max_depth': 10, 'subsample': 0.9939552116505401, 'colsample_bytree': 0.7420007176596146, 'n_estimators': 54}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:00,802] Trial 8 finished with value: 0.7064721286943509 and parameters: {'learning_rate': 0.022164070351454995, 'max_depth': 6, 'subsample': 0.6671863263575195, 'colsample_bytree': 0.7103815182925446, 'n_estimators': 256}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:07,522] Trial 9 finished with value: 0.6995136550692106 and parameters: {'learning_rate': 0.018613439689496847, 'max_depth': 6, 'subsample': 0.609825952253858, 'colsample_bytree': 0.8856628665060381, 'n_estimators': 221}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:15,443] Trial 10 finished with value: 0.6925551814440704 and parameters: {'learning_rate': 0.0011262848684055653, 'max_depth': 8, 'subsample': 0.7995315472551379, 'colsample_bytree': 0.990786517434121, 'n_estimators': 130}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:18,703] Trial 11 finished with value: 0.7151515151515151 and parameters: {'learning_rate': 0.2913357704737845, 'max_depth': 10, 'subsample': 0.9831081391506706, 'colsample_bytree': 0.799082897284735, 'n_estimators': 53}. Best is trial 2 with value: 0.7183688739244295.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:27,186] Trial 12 finished with value: 0.7192667414889636 and parameters: {'learning_rate': 0.07838876154860155, 'max_depth': 10, 'subsample': 0.5069562774397454, 'colsample_bytree': 0.8855807183802153, 'n_estimators': 113}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:33,112] Trial 13 finished with value: 0.7182192293303404 and parameters: {'learning_rate': 0.07095426954214716, 'max_depth': 8, 'subsample': 0.5222837538067142, 'colsample_bytree': 0.921664380977879, 'n_estimators': 110}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:41,251] Trial 14 finished with value: 0.718294051627385 and parameters: {'learning_rate': 0.0901571274804921, 'max_depth': 9, 'subsample': 0.7945285486724984, 'colsample_bytree': 0.8726136909115909, 'n_estimators': 146}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:50,534] Trial 15 finished with value: 0.708492330714553 and parameters: {'learning_rate': 0.006241191240009638, 'max_depth': 10, 'subsample': 0.5067982384159837, 'colsample_bytree': 0.9795013864982471, 'n_estimators': 101}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:54,066] Trial 16 finished with value: 0.7010849233071454 and parameters: {'learning_rate': 0.08879727365430132, 'max_depth': 3, 'subsample': 0.7580888614317118, 'colsample_bytree': 0.8217431686199274, 'n_estimators': 248}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:50:59,815] Trial 17 finished with value: 0.7179947624392069 and parameters: {'learning_rate': 0.13136836941379676, 'max_depth': 9, 'subsample': 0.8692386973144424, 'colsample_bytree': 0.9274039607531397, 'n_estimators': 175}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:03,436] Trial 18 finished with value: 0.7079685746352413 and parameters: {'learning_rate': 0.04877906310686673, 'max_depth': 7, 'subsample': 0.8549537573006285, 'colsample_bytree': 0.6836859805462199, 'n_estimators': 97}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:12,196] Trial 19 finished with value: 0.7015338570894126 and parameters: {'learning_rate': 0.0069741520447416105, 'max_depth': 9, 'subsample': 0.6014665439994374, 'colsample_bytree': 0.7826428485938811, 'n_estimators': 133}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:22,275] Trial 20 finished with value: 0.687317620650954 and parameters: {'learning_rate': 0.0020586440812736632, 'max_depth': 7, 'subsample': 0.7413208296470587, 'colsample_bytree': 0.8489938221554663, 'n_estimators': 240}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:29,845] Trial 21 finished with value: 0.7182192293303403 and parameters: {'learning_rate': 0.09538793677744242, 'max_depth': 9, 'subsample': 0.810365384066661, 'colsample_bytree': 0.8764520282641484, 'n_estimators': 152}. Best is trial 12 with value: 0.7192667414889636.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:41,751] Trial 22 finished with value: 0.7209128320239431 and parameters: {'learning_rate': 0.054592705030006815, 'max_depth': 10, 'subsample': 0.8011404391902123, 'colsample_bytree': 0.9129169820544668, 'n_estimators': 135}. Best is trial 22 with value: 0.7209128320239431.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:50,192] Trial 23 finished with value: 0.7167227833894501 and parameters: {'learning_rate': 0.04970434596872234, 'max_depth': 10, 'subsample': 0.9148186921781485, 'colsample_bytree': 0.9380265339665658, 'n_estimators': 84}. Best is trial 22 with value: 0.7209128320239431.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:51:55,308] Trial 24 finished with value: 0.7162738496071829 and parameters: {'learning_rate': 0.1680244956077731, 'max_depth': 10, 'subsample': 0.7377486273282358, 'colsample_bytree': 0.9367960405848179, 'n_estimators': 121}. Best is trial 22 with value: 0.7209128320239431.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:52:03,884] Trial 25 finished with value: 0.7158249158249158 and parameters: {'learning_rate': 0.03763657581434325, 'max_depth': 8, 'subsample': 0.8271575491206691, 'colsample_bytree': 0.7614298212368913, 'n_estimators': 172}. Best is trial 22 with value: 0.7209128320239431.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:52:14,476] Trial 26 finished with value: 0.7208380097268987 and parameters: {'learning_rate': 0.06698829929352057, 'max_depth': 10, 'subsample': 0.7654400529814206, 'colsample_bytree': 0.8537490957028712, 'n_estimators': 197}. Best is trial 22 with value: 0.7209128320239431.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:52:26,017] Trial 27 finished with value: 0.7232323232323232 and parameters: {'learning_rate': 0.06407845536188769, 'max_depth': 10, 'subsample': 0.6367951683747665, 'colsample_bytree': 0.9069731639862751, 'n_estimators': 186}. Best is trial 27 with value: 0.7232323232323232.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:52:29,660] Trial 28 finished with value: 0.6732510288065845 and parameters: {'learning_rate': 0.010831472548782892, 'max_depth': 4, 'subsample': 0.6313929108617453, 'colsample_bytree': 0.9702580725564613, 'n_estimators': 188}. Best is trial 27 with value: 0.7232323232323232.\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n[I 2025-10-05 11:52:32,131] Trial 29 finished with value: 0.7102132435465769 and parameters: {'learning_rate': 0.2592346160827212, 'max_depth': 5, 'subsample': 0.703516987205452, 'colsample_bytree': 0.9090314045953278, 'n_estimators': 226}. Best is trial 27 with value: 0.7232323232323232.\n[I 2025-10-05 11:52:32,132] A new study created in memory with name: no-name-dfd112a4-2e83-4439-980e-7835195c31b9\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Xgboost best params: {'learning_rate': 0.06407845536188769, 'max_depth': 10, 'subsample': 0.6367951683747665, 'colsample_bytree': 0.9069731639862751, 'n_estimators': 186}\n\nEarly stopping occurred at epoch 3 with best_epoch = 1 and best_val_0_accuracy = 0.45754\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.4624\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 3 with best_epoch = 1 and best_val_0_accuracy = 0.4624\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.49532\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.46165\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:53:11,548] Trial 0 finished with value: 0.467863823419379 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 8, 'n_steps': 9, 'gamma': 1.4, 'lambda_sparse': 1.2810870558994255e-05, 'lr': 0.0005783072733326021, 'momentum': 0.38209813425243006}. Best is trial 0 with value: 0.467863823419379.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 16 with best_epoch = 14 and best_val_0_accuracy = 0.63599\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 3 with best_epoch = 1 and best_val_0_accuracy = 0.54059\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.62926\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.60494\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.59035\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:54:02,872] Trial 1 finished with value: 0.6002244668911336 and parameters: {'optimizer_fn': 'adamw', 'n_d': 40, 'n_a': 48, 'n_steps': 8, 'gamma': 1.2, 'lambda_sparse': 7.213175402766765e-06, 'lr': 0.0028290314304875814, 'momentum': 0.032308854886727446}. Best is trial 1 with value: 0.6002244668911336.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.50019\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 24 with best_epoch = 22 and best_val_0_accuracy = 0.52338\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 36 with best_epoch = 34 and best_val_0_accuracy = 0.56117\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 37 with best_epoch = 35 and best_val_0_accuracy = 0.56266\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 40 with best_epoch = 38 and best_val_0_accuracy = 0.54994\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:55:33,860] Trial 2 finished with value: 0.5394687616909839 and parameters: {'optimizer_fn': 'adamw', 'n_d': 16, 'n_a': 24, 'n_steps': 4, 'gamma': 1.7000000000000002, 'lambda_sparse': 2.568311110322362e-05, 'lr': 0.000192518694398692, 'momentum': 0.03860644245563545}. Best is trial 1 with value: 0.6002244668911336.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.54059\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.55668\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 25 with best_epoch = 23 and best_val_0_accuracy = 0.62364\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 31 with best_epoch = 29 and best_val_0_accuracy = 0.63038\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 32 with best_epoch = 30 and best_val_0_accuracy = 0.61878\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:56:44,046] Trial 3 finished with value: 0.5940142162364385 and parameters: {'optimizer_fn': 'adamw', 'n_d': 56, 'n_a': 56, 'n_steps': 4, 'gamma': 1.8, 'lambda_sparse': 0.0002872813374366066, 'lr': 0.00033030011053185825, 'momentum': 0.19320979928006835}. Best is trial 1 with value: 0.6002244668911336.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.57464\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.56266\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 13 with best_epoch = 11 and best_val_0_accuracy = 0.58249\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.58848\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 4 with best_epoch = 2 and best_val_0_accuracy = 0.47811\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:57:53,884] Trial 4 finished with value: 0.5572764683875795 and parameters: {'optimizer_fn': 'adamw', 'n_d': 40, 'n_a': 24, 'n_steps': 9, 'gamma': 1.7000000000000002, 'lambda_sparse': 0.0002207048886558005, 'lr': 0.0008927937879613778, 'momentum': 0.042865588265540946}. Best is trial 1 with value: 0.6002244668911336.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.59297\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 27 with best_epoch = 25 and best_val_0_accuracy = 0.59596\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 27 with best_epoch = 25 and best_val_0_accuracy = 0.61728\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 17 with best_epoch = 15 and best_val_0_accuracy = 0.58324\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.58661\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:58:48,217] Trial 5 finished with value: 0.5952113729891507 and parameters: {'optimizer_fn': 'adamw', 'n_d': 24, 'n_a': 64, 'n_steps': 3, 'gamma': 1.1, 'lambda_sparse': 0.000399780541141052, 'lr': 0.00015808603045837122, 'momentum': 0.24803650733893146}. Best is trial 1 with value: 0.6002244668911336.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.62327\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 24 with best_epoch = 22 and best_val_0_accuracy = 0.63374\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 23 with best_epoch = 21 and best_val_0_accuracy = 0.64198\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.64385\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 20 with best_epoch = 18 and best_val_0_accuracy = 0.63187\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 11:59:38,981] Trial 6 finished with value: 0.6349420127197904 and parameters: {'optimizer_fn': 'adam', 'n_d': 24, 'n_a': 32, 'n_steps': 3, 'gamma': 1.1, 'lambda_sparse': 0.00022558199350427763, 'lr': 0.0005766578290643896, 'momentum': 0.04627368059645296}. Best is trial 6 with value: 0.6349420127197904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 6 with best_epoch = 4 and best_val_0_accuracy = 0.57351\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.62851\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 16 with best_epoch = 14 and best_val_0_accuracy = 0.65956\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.60382\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.64609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:00:18,516] Trial 7 finished with value: 0.6222970445192668 and parameters: {'optimizer_fn': 'adam', 'n_d': 40, 'n_a': 32, 'n_steps': 4, 'gamma': 1.1, 'lambda_sparse': 1.0762839822389636e-06, 'lr': 0.0019296637612810177, 'momentum': 0.38326000072952016}. Best is trial 6 with value: 0.6349420127197904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.54433\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.60269\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.6315\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 6 with best_epoch = 4 and best_val_0_accuracy = 0.57127\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.581\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:01:10,201] Trial 8 finished with value: 0.586157875046764 and parameters: {'optimizer_fn': 'adam', 'n_d': 32, 'n_a': 40, 'n_steps': 9, 'gamma': 1.8, 'lambda_sparse': 3.453613365753623e-05, 'lr': 0.004570596194251496, 'momentum': 0.318235080079201}. Best is trial 6 with value: 0.6349420127197904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.57538\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.53648\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 17 with best_epoch = 15 and best_val_0_accuracy = 0.59783\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.59222\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.53461\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:02:06,699] Trial 9 finished with value: 0.5673026561915451 and parameters: {'optimizer_fn': 'adam', 'n_d': 16, 'n_a': 24, 'n_steps': 6, 'gamma': 2.0, 'lambda_sparse': 1.4189949067682316e-06, 'lr': 0.0016454827807962796, 'momentum': 0.08899752570259087}. Best is trial 6 with value: 0.6349420127197904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.61953\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 6 with best_epoch = 4 and best_val_0_accuracy = 0.59259\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.62926\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.60045\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.63075\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:02:42,863] Trial 10 finished with value: 0.6145155256266367 and parameters: {'optimizer_fn': 'adam', 'n_d': 64, 'n_a': 8, 'n_steps': 6, 'gamma': 1.4, 'lambda_sparse': 0.00011250583270910757, 'lr': 0.009927715598749396, 'momentum': 0.1508102019206427}. Best is trial 6 with value: 0.6349420127197904.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 13 with best_epoch = 11 and best_val_0_accuracy = 0.64534\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 16 with best_epoch = 14 and best_val_0_accuracy = 0.63861\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.66031\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.60606\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.63562\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:03:17,435] Trial 11 finished with value: 0.6371866816311261 and parameters: {'optimizer_fn': 'adam', 'n_d': 48, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0, 'lambda_sparse': 1.014282845547437e-06, 'lr': 0.0017548777689154827, 'momentum': 0.3962503235363014}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.64534\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.58212\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.60494\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 20 with best_epoch = 18 and best_val_0_accuracy = 0.64235\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.61167\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:03:55,052] Trial 12 finished with value: 0.6172839506172838 and parameters: {'optimizer_fn': 'adam', 'n_d': 56, 'n_a': 40, 'n_steps': 3, 'gamma': 1.0, 'lambda_sparse': 0.0007298897795182789, 'lr': 0.0005235338069109663, 'momentum': 0.27506389286574917}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.58062\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.6401\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 16 with best_epoch = 14 and best_val_0_accuracy = 0.63374\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 17 with best_epoch = 15 and best_val_0_accuracy = 0.63599\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 20 with best_epoch = 18 and best_val_0_accuracy = 0.63786\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:04:52,705] Trial 13 finished with value: 0.6256640478862702 and parameters: {'optimizer_fn': 'adam', 'n_d': 48, 'n_a': 32, 'n_steps': 5, 'gamma': 1.3, 'lambda_sparse': 3.657784092148186e-06, 'lr': 0.0009916242994469234, 'momentum': 0.13194062383771404}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.61092\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.60456\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.63749\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.62813\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 13 with best_epoch = 11 and best_val_0_accuracy = 0.63075\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:05:42,775] Trial 14 finished with value: 0.6223718668163112 and parameters: {'optimizer_fn': 'adam', 'n_d': 24, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0, 'lambda_sparse': 8.544544810225027e-05, 'lr': 0.00401665658062188, 'momentum': 0.31142671368947883}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 17 with best_epoch = 15 and best_val_0_accuracy = 0.59334\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 23 with best_epoch = 21 and best_val_0_accuracy = 0.61691\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.61915\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 20 with best_epoch = 18 and best_val_0_accuracy = 0.6214\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.53685\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:06:24,605] Trial 15 finished with value: 0.5975308641975309 and parameters: {'optimizer_fn': 'adam', 'n_d': 32, 'n_a': 48, 'n_steps': 3, 'gamma': 1.2, 'lambda_sparse': 0.000957715681738231, 'lr': 0.0002931863461517371, 'momentum': 0.19961968633332058}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.59409\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.5997\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_accuracy = 0.60606\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.61317\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.59708\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:06:59,809] Trial 16 finished with value: 0.602020202020202 and parameters: {'optimizer_fn': 'adam', 'n_d': 48, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 6.248171972341212e-05, 'lr': 0.0016125781536280833, 'momentum': 0.12228888886633374}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.36139\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.36102\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.36177\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.36214\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.36102\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:07:11,488] Trial 17 finished with value: 0.36146651702207266 and parameters: {'optimizer_fn': 'adam', 'n_d': 24, 'n_a': 48, 'n_steps': 5, 'gamma': 1.0, 'lambda_sparse': 2.40989171317117e-06, 'lr': 0.00011150021734869546, 'momentum': 0.33918857541984737}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.59035\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.49345\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.5664\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 23 with best_epoch = 21 and best_val_0_accuracy = 0.61205\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.52712\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:08:10,171] Trial 18 finished with value: 0.5578750467639356 and parameters: {'optimizer_fn': 'adam', 'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.2, 'lambda_sparse': 1.0018633691025844e-05, 'lr': 0.0005856823490706882, 'momentum': 0.26190307661304685}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_accuracy = 0.56304\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.55181\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.64272\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.54807\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 4 with best_epoch = 2 and best_val_0_accuracy = 0.54545\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:08:51,336] Trial 19 finished with value: 0.5702207257762814 and parameters: {'optimizer_fn': 'adam', 'n_d': 48, 'n_a': 40, 'n_steps': 10, 'gamma': 1.3, 'lambda_sparse': 2.4576401501783412e-05, 'lr': 0.005484536659315395, 'momentum': 0.08330385253614832}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.61018\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.62252\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 16 with best_epoch = 14 and best_val_0_accuracy = 0.62215\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 23 with best_epoch = 21 and best_val_0_accuracy = 0.63562\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.6401\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:09:48,094] Trial 20 finished with value: 0.6261129816685373 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.1, 'lambda_sparse': 0.00014001514251944395, 'lr': 0.002355614114554339, 'momentum': 0.17577514691535456}. Best is trial 11 with value: 0.6371866816311261.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 34 with best_epoch = 32 and best_val_0_accuracy = 0.65021\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 29 with best_epoch = 27 and best_val_0_accuracy = 0.6517\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.62701\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 27 with best_epoch = 25 and best_val_0_accuracy = 0.64721\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.61654\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:10:46,726] Trial 21 finished with value: 0.6385334829779274 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 16, 'n_steps': 3, 'gamma': 1.1, 'lambda_sparse': 0.00016706775141047962, 'lr': 0.0023155057559813676, 'momentum': 0.17947128572980492}. Best is trial 21 with value: 0.6385334829779274.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.60943\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 27 with best_epoch = 25 and best_val_0_accuracy = 0.62776\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 29 with best_epoch = 27 and best_val_0_accuracy = 0.65544\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.62851\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 25 with best_epoch = 23 and best_val_0_accuracy = 0.6214\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:11:42,308] Trial 22 finished with value: 0.6285072951739619 and parameters: {'optimizer_fn': 'adam', 'n_d': 16, 'n_a': 24, 'n_steps': 3, 'gamma': 1.0, 'lambda_sparse': 0.00048780781624655287, 'lr': 0.0007791639718404806, 'momentum': 0.09636971585777249}. Best is trial 21 with value: 0.6385334829779274.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.63262\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 27 with best_epoch = 25 and best_val_0_accuracy = 0.64759\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.66554\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.63449\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 23 with best_epoch = 21 and best_val_0_accuracy = 0.6401\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:12:36,438] Trial 23 finished with value: 0.6440703329592219 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 32, 'n_steps': 3, 'gamma': 1.1, 'lambda_sparse': 0.00019251047365347084, 'lr': 0.0012922815951897887, 'momentum': 0.23165072255889047}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 28 with best_epoch = 26 and best_val_0_accuracy = 0.61504\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 22 with best_epoch = 20 and best_val_0_accuracy = 0.6098\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 20 with best_epoch = 18 and best_val_0_accuracy = 0.61279\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.58997\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 19 with best_epoch = 17 and best_val_0_accuracy = 0.578\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:13:36,304] Trial 24 finished with value: 0.6011223344556679 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.3, 'lambda_sparse': 5.122084095820457e-05, 'lr': 0.0013079162658483725, 'momentum': 0.22501582893095703}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.57426\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.55256\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.60793\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.58997\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.39357\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:14:18,218] Trial 25 finished with value: 0.543658810325477 and parameters: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 8, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.00014475483562947792, 'lr': 0.002890661380551856, 'momentum': 0.2338268535893418}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 13 with best_epoch = 11 and best_val_0_accuracy = 0.64422\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.61354\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.68051\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.66779\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 7 with best_epoch = 5 and best_val_0_accuracy = 0.61242\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:14:48,928] Trial 26 finished with value: 0.6436962214739992 and parameters: {'optimizer_fn': 'adamw', 'n_d': 16, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2, 'lambda_sparse': 4.36071857887773e-06, 'lr': 0.0074855649650339235, 'momentum': 0.2927371272606285}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 10 with best_epoch = 8 and best_val_0_accuracy = 0.63636\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 2 with best_epoch = 0 and best_val_0_accuracy = 0.55668\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.65619\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 4 with best_epoch = 2 and best_val_0_accuracy = 0.59035\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 7 with best_epoch = 5 and best_val_0_accuracy = 0.61392\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:15:11,716] Trial 27 finished with value: 0.6106995884773662 and parameters: {'optimizer_fn': 'adamw', 'n_d': 16, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2, 'lambda_sparse': 4.14045783539195e-06, 'lr': 0.009591466244084667, 'momentum': 0.28781103944929254}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 12 with best_epoch = 10 and best_val_0_accuracy = 0.62589\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 6 with best_epoch = 4 and best_val_0_accuracy = 0.60344\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.65619\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 14 with best_epoch = 12 and best_val_0_accuracy = 0.64422\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 6 with best_epoch = 4 and best_val_0_accuracy = 0.57613\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:15:38,448] Trial 28 finished with value: 0.6211747100635989 and parameters: {'optimizer_fn': 'adamw', 'n_d': 8, 'n_a': 16, 'n_steps': 3, 'gamma': 1.4, 'lambda_sparse': 1.2592199068117289e-05, 'lr': 0.006182104584962935, 'momentum': 0.17183596322349753}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 15 with best_epoch = 13 and best_val_0_accuracy = 0.58885\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 11 with best_epoch = 9 and best_val_0_accuracy = 0.58399\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 18 with best_epoch = 16 and best_val_0_accuracy = 0.60718\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_accuracy = 0.63823\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEarly stopping occurred at epoch 9 with best_epoch = 7 and best_val_0_accuracy = 0.54733\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[I 2025-10-05 12:16:38,942] Trial 29 finished with value: 0.5931163486719042 and parameters: {'optimizer_fn': 'adamw', 'n_d': 8, 'n_a': 8, 'n_steps': 6, 'gamma': 1.4, 'lambda_sparse': 1.9015553305581375e-05, 'lr': 0.004048571244108934, 'momentum': 0.34689168224738837}. Best is trial 23 with value: 0.6440703329592219.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Tabnet best params: {'optimizer_fn': 'adam', 'n_d': 8, 'n_a': 32, 'n_steps': 3, 'gamma': 1.1, 'lambda_sparse': 0.00019251047365347084, 'lr': 0.0012922815951897887, 'momentum': 0.23165072255889047}\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[27]\tvalid_0's multi_logloss: 0.65149\n\nEarly stopping occurred at epoch 117 with best_epoch = 97 and best_valid_logloss = 0.70884\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_1.zip\nFold 1 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[38]\tvalid_0's multi_logloss: 0.643541\n\nEarly stopping occurred at epoch 178 with best_epoch = 158 and best_valid_logloss = 0.70446\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_2.zip\nFold 2 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[28]\tvalid_0's multi_logloss: 0.661356\n\nEarly stopping occurred at epoch 98 with best_epoch = 78 and best_valid_logloss = 0.72119\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_3.zip\nFold 3 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[27]\tvalid_0's multi_logloss: 0.683708\n\nEarly stopping occurred at epoch 127 with best_epoch = 107 and best_valid_logloss = 0.74865\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_4.zip\nFold 4 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[27]\tvalid_0's multi_logloss: 0.641152\n\nEarly stopping occurred at epoch 95 with best_epoch = 75 and best_valid_logloss = 0.69029\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_5.zip\nFold 5 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[43]\tvalid_0's multi_logloss: 0.617622\n\nEarly stopping occurred at epoch 175 with best_epoch = 155 and best_valid_logloss = 0.65745\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_6.zip\nFold 6 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[34]\tvalid_0's multi_logloss: 0.630218\n\nEarly stopping occurred at epoch 92 with best_epoch = 72 and best_valid_logloss = 0.6879\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_7.zip\nFold 7 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[42]\tvalid_0's multi_logloss: 0.632979\n\nEarly stopping occurred at epoch 63 with best_epoch = 43 and best_valid_logloss = 0.72911\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_8.zip\nFold 8 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[28]\tvalid_0's multi_logloss: 0.672265\n\nEarly stopping occurred at epoch 132 with best_epoch = 112 and best_valid_logloss = 0.70847\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_9.zip\nFold 9 complete.\nTraining until validation scores don't improve for 20 rounds\nEarly stopping, best iteration is:\n[45]\tvalid_0's multi_logloss: 0.656272\nStop training because you reached max_epochs = 200 with best_epoch = 183 and best_valid_logloss = 0.68611\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully saved model at artifacts/tabnet/fold_10.zip\nFold 10 complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL VERIFICATION AND VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STACKING ENSEMBLE - FINAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Check if pipeline ran successfully\n",
        "if 'pipeline_results' not in globals():\n",
        "    print(\"\\n ERROR: pipeline_results not found. The training may not have completed.\")\n",
        "    print(\"Please run the previous cell again.\")\n",
        "else:\n",
        "    print(\"\\n✓ Pipeline completed successfully!\\n\")\n",
        "\n",
        "    # 2. Display Meta Model Performance\n",
        "    print(\"-\"*80)\n",
        "    print(\"META MODEL PERFORMANCE\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Train Accuracy: {pipeline_results['meta_train_accuracy']:.4f}\")\n",
        "    print(f\"Test Accuracy:  {pipeline_results['meta_test_accuracy']:.4f}\")\n",
        "    print(f\"\\nClassification Report:\\n{pipeline_results['classification_report']}\")\n",
        "\n",
        "    # 3. Display Base Model Performance\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"BASE MODEL CROSS-VALIDATION PERFORMANCE\")\n",
        "    print(\"-\"*80)\n",
        "    for model_name, metrics in pipeline_results['fold_metrics'].items():\n",
        "        print(f\"{model_name.upper():12s}: {metrics['mean_accuracy']:.4f} ± {metrics['std_accuracy']:.4f}\")\n",
        "\n",
        "    # 4. Verify saved artifacts\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"SAVED ARTIFACTS\")\n",
        "    print(\"-\"*80)\n",
        "    artifact_root = Path(\"artifacts\")\n",
        "\n",
        "    files_to_check = [\n",
        "        artifact_root / \"best_params.json\",\n",
        "        artifact_root / \"meta_model.joblib\",\n",
        "        artifact_root / \"meta_model_test_predictions.csv\",\n",
        "        artifact_root / \"meta_features_train.npy\",\n",
        "        artifact_root / \"meta_features_test.npy\",\n",
        "        artifact_root / \"meta_test_proba.npy\",\n",
        "    ]\n",
        "\n",
        "    for file_path in files_to_check:\n",
        "        if file_path.exists():\n",
        "            print(f\"✓ {file_path}\")\n",
        "        else:\n",
        "            print(f\"✗ {file_path} - NOT FOUND\")\n",
        "\n",
        "    # Check model folders\n",
        "    for model_type in ['catboost', 'lightgbm', 'xgboost', 'tabnet']:\n",
        "        model_dir = artifact_root / model_type\n",
        "        if model_dir.exists():\n",
        "            num_models = len(list(model_dir.glob('fold_*')))\n",
        "            print(f\"✓ {model_type:12s}: {num_models} fold models\")\n",
        "        else:\n",
        "            print(f\"✗ {model_type:12s}: directory not found\")\n",
        "\n",
        "    # 5. Visualizations\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Plot 1: Base Model Accuracy Comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    models = list(pipeline_results['fold_metrics'].keys())\n",
        "    means = [pipeline_results['fold_metrics'][m]['mean_accuracy'] for m in models]\n",
        "    stds = [pipeline_results['fold_metrics'][m]['std_accuracy'] for m in models]\n",
        "\n",
        "    ax1.bar(models, means, yerr=stds, capsize=5, alpha=0.7, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    ax1.axhline(y=pipeline_results['meta_test_accuracy'], color='red', linestyle='--',\n",
        "                label=f\"Meta Model: {pipeline_results['meta_test_accuracy']:.4f}\")\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_title('Base Model vs Meta Model Performance')\n",
        "    ax1.legend()\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot 2: Confusion Matrix\n",
        "    ax2 = axes[0, 1]\n",
        "    predictions_df = pd.read_csv(artifact_root / \"meta_model_test_predictions.csv\")\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(predictions_df['y_true'], predictions_df['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "    ax2.set_title('Meta Model Confusion Matrix')\n",
        "    ax2.set_ylabel('True Label')\n",
        "    ax2.set_xlabel('Predicted Label')\n",
        "\n",
        "    # Plot 3: Prediction Distribution\n",
        "    ax3 = axes[1, 0]\n",
        "    predictions_df['y_true'].value_counts().sort_index().plot(kind='bar', ax=ax3, alpha=0.7, label='True')\n",
        "    predictions_df['y_pred'].value_counts().sort_index().plot(kind='bar', ax=ax3, alpha=0.7, label='Predicted')\n",
        "    ax3.set_title('Class Distribution: True vs Predicted')\n",
        "    ax3.set_xlabel('Class')\n",
        "    ax3.set_ylabel('Count')\n",
        "    ax3.legend()\n",
        "\n",
        "    # Plot 4: Model Accuracy Distribution\n",
        "    ax4 = axes[1, 1]\n",
        "    all_accuracies = []\n",
        "    model_labels = []\n",
        "    for model_name in models:\n",
        "        # This is a simplification - you'd need actual fold-wise accuracies\n",
        "        mean_acc = pipeline_results['fold_metrics'][model_name]['mean_accuracy']\n",
        "        std_acc = pipeline_results['fold_metrics'][model_name]['std_accuracy']\n",
        "        all_accuracies.append(mean_acc)\n",
        "        model_labels.append(model_name)\n",
        "\n",
        "    all_accuracies.append(pipeline_results['meta_test_accuracy'])\n",
        "    model_labels.append('Meta Model')\n",
        "\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "    ax4.barh(model_labels, all_accuracies, color=colors, alpha=0.7)\n",
        "    ax4.set_xlabel('Accuracy')\n",
        "    ax4.set_title('Model Comparison (Horizontal)')\n",
        "    ax4.set_xlim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(artifact_root / 'model_performance_summary.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n✓ Visualizations saved to {artifact_root / 'model_performance_summary.png'}\")\n",
        "\n",
        "    # 6. Summary Statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    best_base_model = max(pipeline_results['fold_metrics'].items(),\n",
        "                          key=lambda x: x[1]['mean_accuracy'])\n",
        "\n",
        "    print(f\"\\nBest Base Model: {best_base_model[0].upper()}\")\n",
        "    print(f\"  Accuracy: {best_base_model[1]['mean_accuracy']:.4f} ± {best_base_model[1]['std_accuracy']:.4f}\")\n",
        "\n",
        "    improvement = pipeline_results['meta_test_accuracy'] - best_base_model[1]['mean_accuracy']\n",
        "    print(f\"\\nMeta Model Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
        "\n",
        "    if improvement > 0:\n",
        "        print(\"Stacking ensemble improved performance!\")\n",
        "    else:\n",
        "        print(\"Stacking ensemble did not improve over best base model.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"All results have been saved to the 'artifacts' directory.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T12:34:43.725226Z",
          "iopub.execute_input": "2025-10-05T12:34:43.725727Z",
          "iopub.status.idle": "2025-10-05T12:34:45.350949Z",
          "shell.execute_reply.started": "2025-10-05T12:34:43.725702Z",
          "shell.execute_reply": "2025-10-05T12:34:45.350228Z"
        },
        "id": "PpdERLf8boeF",
        "outputId": "06809ee0-979c-4fe6-8063-730af85cbb4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "================================================================================\nSTACKING ENSEMBLE - FINAL RESULTS\n================================================================================\n\n✓ Pipeline completed successfully!\n\n--------------------------------------------------------------------------------\nMETA MODEL PERFORMANCE\n--------------------------------------------------------------------------------\nTrain Accuracy: 0.7279\nTest Accuracy:  0.7373\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0     0.6596    0.7335    0.6946       803\n           1     0.7275    0.7095    0.7184      1332\n           2     0.8087    0.7705    0.7891      1207\n\n    accuracy                         0.7373      3342\n   macro avg     0.7319    0.7378    0.7340      3342\nweighted avg     0.7405    0.7373    0.7382      3342\n\n\n--------------------------------------------------------------------------------\nBASE MODEL CROSS-VALIDATION PERFORMANCE\n--------------------------------------------------------------------------------\nCATBOOST    : 0.7171 ± 0.0117\nLIGHTGBM    : 0.7214 ± 0.0099\nXGBOOST     : 0.7253 ± 0.0123\nTABNET      : 0.6917 ± 0.0148\n\n--------------------------------------------------------------------------------\nSAVED ARTIFACTS\n--------------------------------------------------------------------------------\n✓ artifacts/best_params.json\n✓ artifacts/meta_model.joblib\n✓ artifacts/meta_model_test_predictions.csv\n✓ artifacts/meta_features_train.npy\n✓ artifacts/meta_features_test.npy\n✓ artifacts/meta_test_proba.npy\n✓ catboost    : 10 fold models\n✓ lightgbm    : 10 fold models\n✓ xgboost     : 10 fold models\n✓ tabnet      : 10 fold models\n\n================================================================================\nGENERATING VISUALIZATIONS\n================================================================================\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1500x1200 with 5 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n✓ Visualizations saved to artifacts/model_performance_summary.png\n\n================================================================================\nSUMMARY\n================================================================================\n\nBest Base Model: XGBOOST\n  Accuracy: 0.7253 ± 0.0123\n\nMeta Model Improvement: +0.0120 (+1.20%)\n✓ Stacking ensemble improved performance!\n\n================================================================================\nAll results have been saved to the 'artifacts' directory.\n================================================================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('artifacts', 'zip', '/kaggle/working/artifacts')\n",
        "\n",
        "from IPython.display import FileLink\n",
        "FileLink(r'artifacts.zip')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-05T12:50:29.886068Z",
          "iopub.execute_input": "2025-10-05T12:50:29.886346Z",
          "iopub.status.idle": "2025-10-05T12:50:38.111015Z",
          "shell.execute_reply.started": "2025-10-05T12:50:29.886328Z",
          "shell.execute_reply": "2025-10-05T12:50:38.110402Z"
        },
        "id": "V1Z0qj-OboeG",
        "outputId": "572a8b27-be98-4379-d9d9-813e5530a02c"
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/kaggle/working/artifacts.zip",
            "text/html": "<a href='artifacts.zip' target='_blank'>artifacts.zip</a><br>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ]
}