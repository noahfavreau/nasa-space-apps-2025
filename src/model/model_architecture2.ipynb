{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/noahfavreau/nasa-space-apps-2025/blob/main/model_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib matplotlib numpy optuna pandas lightgbm xgboost catboost pytorch-tabnet scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqSsZp_xdjDY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "RANDOM_SEED = 67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_ROOT = Path(\"artifacts\")\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path) -> Path:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def reshape_for_meta(array: np.ndarray) -> np.ndarray:\n",
    "    array = np.asarray(array)\n",
    "    if array.ndim == 1:\n",
    "        return array.reshape(-1, 1)\n",
    "    return array\n",
    "\n",
    "\n",
    "def persist_model(model, destination: Path, model_name: str) -> Path:\n",
    "    ensure_dir(destination.parent)\n",
    "    if model_name == \"lightgbm\":\n",
    "        file_path = destination.with_suffix(\".txt\")\n",
    "        model.save_model(str(file_path))\n",
    "    elif model_name == \"catboost\":\n",
    "        file_path = destination.with_suffix(\".cbm\")\n",
    "        model.save_model(str(file_path))\n",
    "    elif model_name == \"xgboost\":\n",
    "        file_path = destination.with_suffix(\".json\")\n",
    "        model.save_model(str(file_path))\n",
    "    elif model_name == \"tabnet\":\n",
    "        base_path = str(destination)\n",
    "        if base_path.endswith(\".zip\"):\n",
    "            base_path = base_path[:-4]\n",
    "        model.save_model(base_path)\n",
    "        file_path = Path(f\"{base_path}.zip\")\n",
    "    else:\n",
    "        file_path = destination.with_suffix(\".pkl\")\n",
    "        with open(file_path, \"wb\") as fp:\n",
    "            pickle.dump(model, fp)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def lightgbm_predict_proba(model: lgb.Booster, data):\n",
    "    best_iter = model.best_iteration or model.current_iteration()\n",
    "    return model.predict(data, num_iteration=best_iter)\n",
    "\n",
    "\n",
    "def compute_fold_sample_weight(y_values):\n",
    "    return compute_sample_weight(class_weight=\"balanced\", y=y_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaBGXOupeCap"
   },
   "outputs": [],
   "source": [
    "def train_catboost(X_train, X_val, y_train, y_val, params, fold, sample_weight=None):\n",
    "    model_params = {**params}\n",
    "    model_params.setdefault(\"loss_function\", \"MultiClass\")\n",
    "    model_params.setdefault(\"eval_metric\", \"MultiClass\")\n",
    "    if \"task_type\" not in model_params:\n",
    "        model_params[\"task_type\"] = \"CPU\"\n",
    "    if model_params.get(\"task_type\") == \"GPU\" and \"devices\" not in model_params:\n",
    "        model_params[\"devices\"] = \"0\"\n",
    "    model_params.setdefault(\"verbose\", 0)\n",
    "    model_params[\"random_state\"] = RANDOM_SEED + fold\n",
    "\n",
    "    train_pool = Pool(X_train, y_train, weight=sample_weight)\n",
    "    val_pool = Pool(X_val, y_val)\n",
    "\n",
    "    model = CatBoostClassifier(**model_params)\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        early_stopping_rounds=20,\n",
    "        use_best_model=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_proba = model.predict_proba(X_val)\n",
    "    val_preds = np.argmax(val_proba, axis=1)\n",
    "    accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    return model, val_proba, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgFHm1ENk5G1"
   },
   "outputs": [],
   "source": [
    "def train_lgb(X_train, X_val, y_train, y_val, params, fold, sample_weight=None, num_classes=3):\n",
    "    model_params = {**params}\n",
    "    model_params.setdefault(\"objective\", \"multiclass\")\n",
    "    model_params.setdefault(\"num_class\", num_classes)\n",
    "    model_params.setdefault(\"metric\", \"multi_logloss\")\n",
    "    model_params.setdefault(\"verbose\", -1)\n",
    "    model_params[\"seed\"] = RANDOM_SEED + fold\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weight)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        model_params,\n",
    "        train_data,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20)],\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    val_proba = lightgbm_predict_proba(model, X_val)\n",
    "    val_preds = np.argmax(val_proba, axis=1)\n",
    "    accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    return model, val_proba, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nKka_U8nvif"
   },
   "outputs": [],
   "source": [
    "def train_xgb(X_train, X_val, y_train, y_val, params, fold, sample_weight=None, num_classes=3):\n",
    "    model_params = {**params}\n",
    "    model_params.setdefault(\"objective\", \"multi:softprob\")\n",
    "    model_params.setdefault(\"num_class\", num_classes)\n",
    "    model_params.setdefault(\"eval_metric\", \"mlogloss\")\n",
    "    model_params.setdefault(\"verbosity\", 0)\n",
    "    model_params[\"random_state\"] = RANDOM_SEED + fold\n",
    "    model_params[\"use_label_encoder\"] = False\n",
    "\n",
    "    model = XGBClassifier(**model_params)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        sample_weight=sample_weight,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_proba = model.predict_proba(X_val)\n",
    "    val_preds = np.argmax(val_proba, axis=1)\n",
    "    accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    return model, val_proba, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPNchn4MSNsB"
   },
   "outputs": [],
   "source": [
    "def train_tabnet(X_train, X_val, y_train, y_val, params, fold, sample_weight=None):\n",
    "    model_params = {**params}\n",
    "    model_params.setdefault(\"seed\", RANDOM_SEED + fold)\n",
    "    model_params.setdefault(\"verbose\", 0)\n",
    "\n",
    "    model = TabNetClassifier(**model_params)\n",
    "    model.fit(\n",
    "        X_train.values,\n",
    "        y_train.values,\n",
    "        eval_set=[(X_val.values, y_val.values)],\n",
    "        eval_metric=[\"accuracy\"],\n",
    "        max_epochs=200,\n",
    "        patience=20,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        weights=sample_weight,\n",
    "    )\n",
    "\n",
    "    val_proba = model.predict_proba(X_val.values)\n",
    "    val_preds = np.argmax(val_proba, axis=1)\n",
    "    accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    return model, val_proba, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6L98bGoWC1BF"
   },
   "outputs": [],
   "source": [
    "def objective_catboost(trial, X, y, n_splits=5, random_seed=RANDOM_SEED):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"task_type\": trial.suggest_categorical(\"task_type\", [\"CPU\", \"GPU\"]),\n",
    "        \"loss_function\": \"MultiClass\",\n",
    "        \"eval_metric\": \"MultiClass\",\n",
    "        \"random_seed\": random_seed,\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    if params[\"task_type\"] == \"GPU\":\n",
    "        params[\"devices\"] = \"0\"\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_weights = compute_fold_sample_weight(y_train)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            sample_weight=train_weights,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return float(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loxVLr4yDcfu"
   },
   "outputs": [],
   "source": [
    "def objective_lgb(trial, X, y, num_classes, n_splits=5, random_seed=RANDOM_SEED):\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": num_classes,\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": random_seed,\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_weights = compute_fold_sample_weight(y_train)\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weights)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=20)],\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        preds = np.argmax(lightgbm_predict_proba(model, X_val), axis=1)\n",
    "        accuracies.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return float(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s19NrbWDhuZ"
   },
   "outputs": [],
   "source": [
    "def objective_xgb(trial, X, y, num_classes, n_splits=5, random_seed=RANDOM_SEED):\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": num_classes,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"random_state\": random_seed,\n",
    "        \"verbosity\": 0,\n",
    "        \"use_label_encoder\": False,\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_weights = compute_fold_sample_weight(y_train)\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            sample_weight=train_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return float(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZPh7e2RU0Ji"
   },
   "outputs": [],
   "source": [
    "def objective_tabnet(trial, X, y, n_splits=5, random_seed=RANDOM_SEED):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64, step=8),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64, step=8),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.1),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
    "        \"optimizer_fn\": trial.suggest_categorical(\"optimizer_fn\", [\"adam\", \"adamw\"]),\n",
    "        \"optimizer_params\": {\"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)},\n",
    "        \"momentum\": trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
    "        \"seed\": random_seed,\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_weights = compute_fold_sample_weight(y_train)\n",
    "\n",
    "        model = TabNetClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train.values,\n",
    "            y_train.values,\n",
    "            eval_set=[(X_val.values, y_val.values)],\n",
    "            eval_metric=[\"accuracy\"],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            weights=train_weights,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val.values)\n",
    "        accuracies.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return float(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsMKzppbpKTH"
   },
   "outputs": [],
   "source": [
    "def run_stacking_pipeline(\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    test_size=0.2,\n",
    "    n_trials=30,\n",
    "    base_cv_splits=10,\n",
    "    optuna_cv_splits=5,\n",
    "    artifact_root: Path = ARTIFACT_ROOT,\n",
    "    random_seed: int = RANDOM_SEED,\n",
    "):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X_df = pd.DataFrame(X)\n",
    "    else:\n",
    "        X_df = X.copy()\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        if y.shape[1] != 1:\n",
    "            raise ValueError(\"y must be a 1-D vector.\")\n",
    "        y_series = y.iloc[:, 0].copy()\n",
    "    elif isinstance(y, pd.Series):\n",
    "        y_series = y.copy()\n",
    "    else:\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "    if X_df.shape[0] != len(y_series):\n",
    "        raise ValueError(\"X and y must have the same number of rows.\")\n",
    "    if len(np.unique(y_series)) < 2:\n",
    "        raise ValueError(\"y must contain at least two classes.\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    artifact_root = ensure_dir(Path(artifact_root))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_df,\n",
    "        y_series,\n",
    "        test_size=test_size,\n",
    "        stratify=y_series,\n",
    "        random_state=random_seed,\n",
    "    )\n",
    "\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    objective_wrappers = {\n",
    "        \"catboost\": partial(\n",
    "            objective_catboost,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            n_splits=optuna_cv_splits,\n",
    "            random_seed=random_seed,\n",
    "        ),\n",
    "        \"lightgbm\": partial(\n",
    "            objective_lgb,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            num_classes=num_classes,\n",
    "            n_splits=optuna_cv_splits,\n",
    "            random_seed=random_seed,\n",
    "        ),\n",
    "        \"xgboost\": partial(\n",
    "            objective_xgb,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            num_classes=num_classes,\n",
    "            n_splits=optuna_cv_splits,\n",
    "            random_seed=random_seed,\n",
    "        ),\n",
    "        \"tabnet\": partial(\n",
    "            objective_tabnet,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            n_splits=optuna_cv_splits,\n",
    "            random_seed=random_seed,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    best_params = {}\n",
    "    for model_name, objective_fn in objective_wrappers.items():\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective_fn, n_trials=n_trials)\n",
    "        best_params[model_name] = study.best_params\n",
    "        print(f\"{model_name.title()} best params: {study.best_params}\")\n",
    "\n",
    "    with open(artifact_root / \"best_params.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(best_params, fp, indent=2)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=base_cv_splits, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    oof_predictions = {\n",
    "        \"catboost\": np.zeros((len(y_train), num_classes)),\n",
    "        \"lightgbm\": np.zeros((len(y_train), num_classes)),\n",
    "        \"xgboost\": np.zeros((len(y_train), num_classes)),\n",
    "        \"tabnet\": np.zeros((len(y_train), num_classes)),\n",
    "    }\n",
    "    test_predictions = {\n",
    "        \"catboost\": np.zeros((len(y_test), num_classes)),\n",
    "        \"lightgbm\": np.zeros((len(y_test), num_classes)),\n",
    "        \"xgboost\": np.zeros((len(y_test), num_classes)),\n",
    "        \"tabnet\": np.zeros((len(y_test), num_classes)),\n",
    "    }\n",
    "    fold_accuracies = {model: [] for model in oof_predictions}\n",
    "    saved_model_paths = {model: [] for model in oof_predictions}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        train_weights = compute_fold_sample_weight(y_tr)\n",
    "\n",
    "        cb_model, cb_val_proba, cb_acc = train_catboost(\n",
    "            X_tr,\n",
    "            X_val,\n",
    "            y_tr,\n",
    "            y_val,\n",
    "            params=best_params[\"catboost\"],\n",
    "            fold=fold,\n",
    "            sample_weight=train_weights,\n",
    "        )\n",
    "        oof_predictions[\"catboost\"][val_idx] = cb_val_proba\n",
    "        test_predictions[\"catboost\"] += cb_model.predict_proba(X_test)\n",
    "        fold_accuracies[\"catboost\"].append(cb_acc)\n",
    "        saved_model_paths[\"catboost\"].append(\n",
    "            str(persist_model(cb_model, artifact_root / \"catboost\" / f\"fold_{fold}\", \"catboost\"))\n",
    "        )\n",
    "\n",
    "        lgb_model, lgb_val_proba, lgb_acc = train_lgb(\n",
    "            X_tr,\n",
    "            X_val,\n",
    "            y_tr,\n",
    "            y_val,\n",
    "            params=best_params[\"lightgbm\"],\n",
    "            fold=fold,\n",
    "            sample_weight=train_weights,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        oof_predictions[\"lightgbm\"][val_idx] = lgb_val_proba\n",
    "        test_predictions[\"lightgbm\"] += lightgbm_predict_proba(lgb_model, X_test)\n",
    "        fold_accuracies[\"lightgbm\"].append(lgb_acc)\n",
    "        saved_model_paths[\"lightgbm\"].append(\n",
    "            str(persist_model(lgb_model, artifact_root / \"lightgbm\" / f\"fold_{fold}\", \"lightgbm\"))\n",
    "        )\n",
    "\n",
    "        xgb_model, xgb_val_proba, xgb_acc = train_xgb(\n",
    "            X_tr,\n",
    "            X_val,\n",
    "            y_tr,\n",
    "            y_val,\n",
    "            params=best_params[\"xgboost\"],\n",
    "            fold=fold,\n",
    "            sample_weight=train_weights,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        oof_predictions[\"xgboost\"][val_idx] = xgb_val_proba\n",
    "        test_predictions[\"xgboost\"] += xgb_model.predict_proba(X_test)\n",
    "        fold_accuracies[\"xgboost\"].append(xgb_acc)\n",
    "        saved_model_paths[\"xgboost\"].append(\n",
    "            str(persist_model(xgb_model, artifact_root / \"xgboost\" / f\"fold_{fold}\", \"xgboost\"))\n",
    "        )\n",
    "\n",
    "        tabnet_model, tabnet_val_proba, tabnet_acc = train_tabnet(\n",
    "            X_tr,\n",
    "            X_val,\n",
    "            y_tr,\n",
    "            y_val,\n",
    "            params=best_params[\"tabnet\"],\n",
    "            fold=fold,\n",
    "            sample_weight=train_weights,\n",
    "        )\n",
    "        oof_predictions[\"tabnet\"][val_idx] = tabnet_val_proba\n",
    "        test_predictions[\"tabnet\"] += tabnet_model.predict_proba(X_test.values)\n",
    "        fold_accuracies[\"tabnet\"].append(tabnet_acc)\n",
    "        saved_model_paths[\"tabnet\"].append(\n",
    "            str(persist_model(tabnet_model, artifact_root / \"tabnet\" / f\"fold_{fold}\", \"tabnet\"))\n",
    "        )\n",
    "\n",
    "        print(f\"Fold {fold} complete.\")\n",
    "\n",
    "    for model_name in test_predictions:\n",
    "        test_predictions[model_name] /= base_cv_splits\n",
    "\n",
    "    meta_features_train = np.hstack([reshape_for_meta(oof_predictions[name]) for name in oof_predictions])\n",
    "    meta_features_test = np.hstack([reshape_for_meta(test_predictions[name]) for name in test_predictions])\n",
    "\n",
    "    meta_model = LogisticRegressionCV(\n",
    "        cv=base_cv_splits,\n",
    "        multi_class=\"multinomial\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=random_seed,\n",
    "    )\n",
    "    meta_model.fit(meta_features_train, y_train)\n",
    "\n",
    "    meta_train_preds = meta_model.predict(meta_features_train)\n",
    "    meta_train_acc = accuracy_score(y_train, meta_train_preds)\n",
    "\n",
    "    meta_test_preds = meta_model.predict(meta_features_test)\n",
    "    meta_test_acc = accuracy_score(y_test, meta_test_preds)\n",
    "    meta_test_proba = meta_model.predict_proba(meta_features_test)\n",
    "    meta_report = classification_report(y_test, meta_test_preds, digits=4)\n",
    "\n",
    "    meta_model_path = artifact_root / \"meta_model.joblib\"\n",
    "    joblib.dump(meta_model, meta_model_path)\n",
    "\n",
    "    predictions_df = pd.DataFrame(\n",
    "        {\n",
    "            \"y_true\": y_test.values,\n",
    "            \"y_pred\": meta_test_preds,\n",
    "        }\n",
    "    )\n",
    "    predictions_path = artifact_root / \"meta_model_test_predictions.csv\"\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "    np.save(artifact_root / \"meta_features_train.npy\", meta_features_train)\n",
    "    np.save(artifact_root / \"meta_features_test.npy\", meta_features_test)\n",
    "    np.save(artifact_root / \"meta_test_proba.npy\", meta_test_proba)\n",
    "\n",
    "    summary = {\n",
    "        \"best_params\": best_params,\n",
    "        \"fold_metrics\": {\n",
    "            name: {\n",
    "                \"mean_accuracy\": float(np.mean(scores)),\n",
    "                \"std_accuracy\": float(np.std(scores)),\n",
    "            }\n",
    "            for name, scores in fold_accuracies.items()\n",
    "        },\n",
    "        \"meta_train_accuracy\": float(meta_train_acc),\n",
    "        \"meta_test_accuracy\": float(meta_test_acc),\n",
    "        \"classification_report\": meta_report,\n",
    "        \"saved_models\": saved_model_paths,\n",
    "        \"meta_model_path\": str(meta_model_path),\n",
    "        \"test_predictions_path\": str(predictions_path),\n",
    "    }\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"dataset/combined_imputed_values.csv\")\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    if \"disposition\" not in df.columns:\n",
    "        raise KeyError(\"Column 'disposition' is missing from the dataset.\")\n",
    "    X = df.drop(columns=[\"disposition\"])\n",
    "    y = df[\"disposition\"]\n",
    "    print(f\"Loaded dataset with shape {df.shape}.\")\n",
    "else:\n",
    "    print(\"Dataset 'combined_imputed_values.csv' not found. Define X and y manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGCwUqMjuQys"
   },
   "outputs": [],
   "source": [
    "if \"X\" in globals() and \"y\" in globals():\n",
    "    pipeline_results = run_stacking_pipeline(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        n_trials=30,\n",
    "        base_cv_splits=10,\n",
    "        optuna_cv_splits=5,\n",
    "        artifact_root=ARTIFACT_ROOT,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    pipeline_results\n",
    "else:\n",
    "    print(\"Define X (features) and y (target) before running the stacking pipeline.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFTN0UV6Iy98/bxKBK9jtZ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
