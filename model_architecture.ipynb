{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUDfdJ4eSWyONZRUTW9/gZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahfavreau/nasa-space-apps-2025/blob/main/model_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UqSsZp_xdjDY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from catboost import CatBoostClassifier, cv, Pool\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from xgboost import plot_importance, XGBClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "import optuna\n",
        "\n",
        "RANDOM_SEED = 67"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_catboost(X_train, X_val, y_train, y_val, params, fold):\n",
        "  params.update({\n",
        "      'task_type': \"GPU\",\n",
        "      'devices' : '0',\n",
        "      'loss_function' : 'MultiClass',\n",
        "      'eval_metric' : 'MultiClass',\n",
        "      'random_state' : RANDOM_SEED + fold,\n",
        "      'verbose' : 0\n",
        "  })\n",
        "\n",
        "  model = CatBoostClassifier(**params)\n",
        "\n",
        "  train_pool = Pool(X_train, y_train)\n",
        "  val_pool = Pool(X_val, y_val)\n",
        "\n",
        "  model.fit(train_pool, eval_set=val_pool,\n",
        "            early_stopping_rounds=20,\n",
        "            use_best_model=True)\n",
        "\n",
        "  predictions = model.predict(X_val)\n",
        "  accuracy = accuracy_score(y_val, predictions)\n",
        "\n",
        "  return model, predictions, accuracy"
      ],
      "metadata": {
        "id": "CaBGXOupeCap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lgb(X_train, X_val, y_train, y_val, params, fold):\n",
        "  params.update({\n",
        "    'objective' : 'multiclass',\n",
        "    'num_class' : 3,\n",
        "    'metric' : \"multi_logloss\",\n",
        "    'verbose' : -1,\n",
        "    'seed' : RANDOM_SEED + fold\n",
        "  })\n",
        "\n",
        "  train_data = lgb.Dataset(X_train, label=y_train)\n",
        "  val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "  model = lgb.train(params,\n",
        "                      train_data,\n",
        "                      num_boost_round=200,\n",
        "                      callbacks=[lgb.early_stopping(stopping_rounds=20)],\n",
        "                      valid_sets=[val_data]\n",
        "                        )\n",
        "\n",
        "  preds_proba = model.predict(X_val,\n",
        "                           num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "  predictions = np.argmax(preds_proba, axis=1)\n",
        "  accuracy = accuracy_score(y_val, predictions)\n",
        "\n",
        "  return model, predictions, accuracy"
      ],
      "metadata": {
        "id": "wgFHm1ENk5G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgb(X_train, X_val, y_train, y_val, params, fold):\n",
        "  params.update({\n",
        "      'objective' : 'multi:softmax',\n",
        "      'num_class': 3,\n",
        "      'eval_metric' : 'mlogloss',\n",
        "      'use_label_encoder' : False,\n",
        "      'random_state' : RANDOM_SEED + fold,\n",
        "      'verbosity' : 0\n",
        "  })\n",
        "\n",
        "  xgb_model = XGBClassifier(**params)\n",
        "\n",
        "  xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
        "                early_stopping_rounds=20)\n",
        "\n",
        "  predictions = xgb_model.predict(X_val)\n",
        "  accuracy = accuracy_score(y_val, predictions)\n",
        "\n",
        "  return xgb_model, predictions, accuracy"
      ],
      "metadata": {
        "id": "0nKka_U8nvif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tabnet(X_train, X_val, y_train, y_val, params, fold):\n",
        "\n",
        "  params.update({\n",
        "      'seed' : RANDOM_SEED + fold,\n",
        "      'verbose' : 0\n",
        "  })\n",
        "\n",
        "  tabnet_model = TabNetClassifier(**params)\n",
        "\n",
        "  tabnet_model.fit(X_train.values, y_train.values,\n",
        "                   eval_set=[(X_val.values, y_val.values)],\n",
        "                   eval_metric=[\"accuracy\"],\n",
        "                   max_epochs=200,\n",
        "                   patience=20,\n",
        "                   batch_size=1024,\n",
        "                   virtual_batch_size=128\n",
        "                   )\n",
        "\n",
        "  predictions = tabnet_model.predict(X_val.values)\n",
        "  accuracy = accuracy_score(y_val, predictions)\n",
        "\n",
        "  return tabnet_model, predictions, accuracy\n"
      ],
      "metadata": {
        "id": "OPNchn4MSNsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_catboost(trial):\n",
        "    params = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
        "        \"task_type\": \"GPU\",\n",
        "        \"devices\": \"0\",\n",
        "        \"loss_function\": \"MultiClass\",\n",
        "        \"random_seed\": RANDOM_SEED\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        model = CatBoostClassifier(**params, verbose=0)\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=20, verbose=0)\n",
        "        preds = model.predict(X_val)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return np.mean(accuracies)\n"
      ],
      "metadata": {
        "id": "6L98bGoWC1BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_lgb(trial):\n",
        "    params = {\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"num_class\": 3,\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train)\n",
        "        val_data = lgb.Dataset(X_val, label=y_val)\n",
        "        model = lgb.train(params, train_data, valid_sets=[val_data], early_stopping_rounds=20, verbose_eval=False)\n",
        "\n",
        "        preds = np.argmax(model.predict(X_val), axis=1)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return np.mean(accuracies)\n"
      ],
      "metadata": {
        "id": "loxVLr4yDcfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        \"objective\": \"multi:softmax\",\n",
        "        \"num_class\": 3,\n",
        "        \"eval_metric\": \"mlogloss\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "        \"random_state\": RANDOM_SEED\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        model = XGBClassifier(**params, use_label_encoder=False)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_val)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return np.mean(accuracies)\n"
      ],
      "metadata": {
        "id": "-s19NrbWDhuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_tabnet(trial):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64, step=8),\n",
        "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64, step=8),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.1),\n",
        "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
        "        \"optimizer_fn\": trial.suggest_categorical(\"optimizer_fn\", [\"adam\", \"adamw\"]),\n",
        "        \"optimizer_params\": dict(\n",
        "            lr=trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "        ),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
        "        \"seed\": RANDOM_SEED,\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        model = TabNetClassifier(**params)\n",
        "\n",
        "        model.fit(\n",
        "            X_train.values, y_train.values,\n",
        "            eval_set=[(X_val.values, y_val.values)],\n",
        "            eval_metric=[\"accuracy\"],\n",
        "            max_epochs=200,\n",
        "            patience=20,\n",
        "            batch_size=1024,\n",
        "            virtual_batch_size=128\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val.values)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return np.mean(accuracies)"
      ],
      "metadata": {
        "id": "lZPh7e2RU0Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = None\n",
        "y = None\n",
        "\n",
        "catboost_fold_accuracies = []\n",
        "xgb_fold_accuracies = []\n",
        "lgb_fold_accuracies = []\n",
        "tabnet_fold_accuracies = []\n",
        "\n",
        "catboost_oof_preds = np.zeros(len(X))\n",
        "xgb_oof_preds = np.zeros(len(X))\n",
        "lgb_oof_preds = np.zeros(len(X))\n",
        "tabnet_oof_preds = np.zeros(len(X))\n",
        "\n",
        "study_cat = optuna.create_study(direction=\"maximize\")\n",
        "study_cat.optimize(objective_catboost, n_trials=30)\n",
        "print(\"Best CatBoost params:\", study_cat.best_params)\n",
        "\n",
        "study_lgb = optuna.create_study(direction=\"maximize\")\n",
        "study_lgb.optimize(objective_lgb, n_trials=30)\n",
        "print(\"Best LightGBM params:\", study_lgb.best_params)\n",
        "\n",
        "study_xgb = optuna.create_study(direction=\"maximize\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=30)\n",
        "print(\"Best XGBoost params:\", study_xgb.best_params)\n",
        "\n",
        "study_tabnet = optuna.create_study(direction=\"maximize\")\n",
        "study_tabnet.optimize(objective_tabnet, n_trials=30)\n",
        "print(\"Best Tabnet params:\", study_tabnet.best_params)\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Train CatBoost with best params\n",
        "    cb_model, cb_preds, cb_acc = train_catboost(\n",
        "        X_train, X_val, y_train, y_val,\n",
        "        study_cat.best_params,  # From Optuna\n",
        "        fold_idx\n",
        "    )\n",
        "    catboost_fold_accuracies.append(cb_acc)\n",
        "    catboost_oof_preds[val_idx] = cb_preds\n",
        "    print(f\"CatBoost Fold {fold_idx}: {cb_acc:.4f}\")\n",
        "\n",
        "    lgb_model, lgb_preds, lgb_acc = train_lgb(\n",
        "        X_train, X_val, y_train, y_val,\n",
        "        study_lgb.best_params,\n",
        "        fold_idx\n",
        "    )\n",
        "    lgb_fold_accuracies.append(lgb_acc)\n",
        "    lgb_oof_preds[val_idx] =lgb_preds\n",
        "    print(f\"LightGBM Fold {fold_idx}: {lgb_acc:.4f}\")\n",
        "\n",
        "    xgb_model, xgb_preds, xgb_acc = train_xgb(\n",
        "        X_train, X_val, y_train, y_val,\n",
        "        study_xgb.best_params,\n",
        "        fold_idx\n",
        "    )\n",
        "    xgb_fold_accuracies.append(xgb_acc)\n",
        "    xgb_oof_preds[val_idx] = xgb_preds\n",
        "    print(f\"XGBoost Fold {fold_idx}: {xgb_acc:.4f}\")\n",
        "\n",
        "    tabnet_model, tabnet_preds, tabnet_acc = train_tabnet(\n",
        "        X_train, X_val, y_train, y_val,\n",
        "        study_tabnet.best_params,\n",
        "        fold_idx\n",
        "    )\n",
        "    tabnet_fold_accuracies.append(tabnet_acc)\n",
        "    tabnet_oof_preds[val_idx] = tabnet_preds\n",
        "    print(f\"TabNet Fold {fold_idx}: {tabnet_acc:.4f}\")\n",
        "\n",
        "\n",
        "catboost_mean_accuracy = np.mean(catboost_fold_accuracies)\n",
        "catboost_std_accuracy = np.std(catboost_fold_accuracies)\n",
        "print(f\"CatBoost : {catboost_mean_accuracy:.4f} (+/- {catboost_std_accuracy:.4f})\")\n",
        "\n",
        "lgb_mean_accuracy = np.mean(lgb_fold_accuracies)\n",
        "lgb_std_accuracy = np.std(lgb_fold_accuracies)\n",
        "print(f\"LGBoost : {lgb_mean_accuracy:.4f} (+/- {lgb_std_accuracy:.4f})\")\n",
        "\n",
        "xgb_mean_accuracy = np.mean(xgb_fold_accuracies)\n",
        "xgb_std_accuracy = np.std(xgb_fold_accuracies)\n",
        "print(f\"XGBoost : {xgb_mean_accuracy:.4f} (+/- {xgb_std_accuracy:.4f})\")\n",
        "\n",
        "tabnet_mean_accuracy = np.mean(tabnet_fold_accuracies)\n",
        "tabnet_std_accuracy = np.std(tabnet_fold_accuracies)\n",
        "print(f\"Tabnet : {tabnet_mean_accuracy:.4f} (+/- {tabnet_std_accuracy:.4f})\")"
      ],
      "metadata": {
        "id": "VsMKzppbpKTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}